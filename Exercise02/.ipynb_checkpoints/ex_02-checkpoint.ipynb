{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Statistik und Machine Learning: Assignment 2\n",
    "\n",
    "The exercises on this sheet are graded by a maximum of 10 points. You will be asked to implement several functions.\n",
    "\n",
    "Team work is not allowed. Everybody implements his/her own code. Discussing issues with others is fine, sharing code with others is not. \n",
    "\n",
    "If you use any code fragments found on the Internet, make sure you reference them properly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 -- Density Estimation\n",
    "\n",
    "The histogram is considered the simplest form of non-parametric density estimation. To plot a histogram we divide the sample space into a number of bins and approximate the density at the center of each bin by the fraction of points in the training data that fall into the corresponding bin. \n",
    "\n",
    "The histogram requires two parameters to be defined: **bin width** and **starting position**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm, multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random data to work with\n",
    "np.random.seed(19)\n",
    "x1 = np.random.normal(0, 2, size = 2000)\n",
    "x2 = np.random.normal(10, 5, size = 2000)\n",
    "data = [x1, x2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data defined above, plot histograms as shown in figure. \n",
    "\n",
    "(a) Write the approximate bin size for generating these histograms.\n",
    "\n",
    "(b) How does the bin size affect the plot? Also, write the drawbacks of using histogram for density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='img/hist_1.png'></td><td><img src='img/hist_2.png'></td><td><img src='img/hist_3.png'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='img/hist_1.png'></td><td><img src='img/hist_2.png'></td><td><img src='img/hist_3.png'></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data, bins):\n",
    "    for x in data:\n",
    "        plt.hist(x, bins = bins, normed = True, alpha = 0.6)\n",
    "\n",
    "    plt.xlim(-10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel density estimate is:\n",
    "\n",
    "$$ \\textit{Pr}(x) = \\frac{1}{Nh^d} \\sum_{i=1}^{N} \\phi_{h}(x - x_{i}) $$\n",
    "\n",
    "where $\\phi_h$ is the kernel function, $h$ is the bandwidth of the kernel. \n",
    "\n",
    "(c) Implement the functions `kernel_density_estimation` and `ker_g` to calculate the density for the above two distributions. Also calculate the true density and plot it for bandwidth values $[0.3,1, 5]$\n",
    "\n",
    "You cannot use already implemented functions for `kernel density estimation` or gaussian kernel in python libraries but you can test if your implementation is correct using `sklearn.neighbors.KernelDensity`. \n",
    "Other helpful functions that you might need: `np.linspace`, `scipy.stats.norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ker_g(x,h):\n",
    "    #define gaussian kernel here\n",
    "    phi = 1/np.sqrt(2*np.pi)*np.exp(-0.5*(x/h)**2)\n",
    "    \n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_density_estimation(x, x_grid, bandwidth, ker):\n",
    "    #define kde function here\n",
    "    N = x.shape[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for kernel density estimation\n",
    "# sample points from the distributions\n",
    "x = np.concatenate([norm(0, 2).rvs(400),\n",
    "                    norm(10, 5).rvs(400)])\n",
    "x_grid = np.linspace(start = -10, stop = 20, num = 200)\n",
    "\n",
    "#calculate the true density for the two mixtures. Assume both distribution to have equal weight\n",
    "pdf_true = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the density estimations along with histogram and true pdf\n",
    "fig, ax = plt.subplots()\n",
    "for bandwidth in [0.3,1, 5]:\n",
    "    ax.plot(x_grid, kernel_density_estimation(x, x_grid, bandwidth, ker_g),\n",
    "            label='bw={0}'.format(bandwidth), linewidth=3, alpha=0.5)\n",
    "ax.plot(x_grid, pdf_true, label='true pdf', linewidth=3, alpha=0.5)\n",
    "ax.hist(x, 100, fc='gray', histtype='stepfilled', alpha=0.3, normed=True)\n",
    "ax.set_xlim(-10, 20)\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your function using following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def kde_1(x, x_grid, bandwidth=0.2, **kwargs):\n",
    "    \"\"\"Kernel Density Estimation with Scikit-learn\"\"\"\n",
    "    kde_skl = KernelDensity(kernel = 'gaussian', bandwidth=bandwidth, **kwargs)\n",
    "    kde_skl.fit(x[:, np.newaxis])\n",
    "    # score_samples() returns the log-likelihood of the samples\n",
    "    log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n",
    "    return np.exp(log_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for bandwidth in [0.3,1, 5]:\n",
    "    ax.plot(x_grid, kde_1(x, x_grid, bandwidth=bandwidth),\n",
    "            label='bw={0}'.format(bandwidth), linewidth=3, alpha=0.5)\n",
    "ax.plot(x_grid, pdf_true, label='true pdf', linewidth=3, alpha=0.5)\n",
    "ax.hist(x, 100, fc='gray', histtype='stepfilled', alpha=0.3, normed=True)\n",
    "ax.set_xlim(-10, 20)\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Observe the effect of different bandwidth values on the estimation and describe it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Complete the kernel functions below and plot them including the gaussian kernel defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential kernel \n",
    "def ker_exp(x,h):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epanechnikov kernel\n",
    "def ker_epv(x,h):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tophat kernel \n",
    "def ker_tophat(x,h):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the kernels\n",
    "u = np.linspace(-5, 5, 10000)\n",
    "h = 1\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(u, ker_g(u, h), '-', c='black', lw=1, label='Gaussian')\n",
    "ax.plot(u, ker_exp(u,h), '-', c=((0, 0.38, 1)), lw=1, label='Exponential')\n",
    "ax.plot(u, ker_tophat(u,h), '-', c=((0.30, 1, 0.67)), lw=1, label='Top-hat')\n",
    "ax.plot(u, ker_epv(u,h), '-', c=((1, 0.48, 0)), lw=1, label='Epanechnikov')\n",
    "\n",
    "ax.legend(loc=1)\n",
    "\n",
    "ax.set_xlabel('$u$')\n",
    "ax.set_ylabel('$K(u)$')\n",
    "\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 -- Gaussian Mixture Model and Expectation-Maximization Algorithm\n",
    "\n",
    "In a Gaussian Mixture Model (GMM) the probability of observing a variable $\\mathbf{x}$ can be written as:\n",
    "\n",
    "$$ p(\\mathbf{x}) = \\sum_{m=1}^M \\alpha_m \\mathcal{N}(\\mathbf{x}|\\mu_m,\\Sigma_m) $$\n",
    "\n",
    "where $\\sum_{m=1}^M \\alpha_m =1$ and $\\alpha_m \\in [0,1]$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import scipy.stats as st\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 -- GMM Sampling\n",
    "Implement the function `GMM_rvs` which is able to draw samples from a GMM. the function receives as input: the $\\alpha$ parameters as a numpy array, a list containing the mean parameters $\\mu$ (each mean has dimension $D$), a list containing the covariance matrices $\\Sigma$ (each covariance matrix has dimension $D \\times D$), and the number of samples to generate. The function returns a numpy array containing the N random samples, the output should be of shape (N_samples,D)\n",
    "\n",
    "hint: In this exercise you can use, if you want, the following scipy functions:`scipy.stats.multivariate_normal.rvs`, and/or `scipy.stats.uniform.rvs`, and/or `scipy.stats.multinomial.rvs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_rvs(N,alpha,mu_list,cov_list):\n",
    "    \n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your function using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.array([0.25,0.15,0.35,0.25])\n",
    "\n",
    "mu_1 = np.array([0,1.])\n",
    "cov_1 = np.array([[1,-0.8],[-0.8,1]])\n",
    "mu_2 = np.array([-1.,-1.5])\n",
    "cov_2 = np.array([[0.25,-0.09],[-0.09,0.25]])\n",
    "mu_3 = np.array([1,-1.5])\n",
    "cov_3 = np.array([[0.1,0.0],[0.0,0.1]])\n",
    "mu_4 = np.array([0,1.25])\n",
    "cov_4 = np.array([[0.95,0.8],[0.8,0.95]])\n",
    "\n",
    "mu_list = [mu_1, mu_2, mu_3, mu_4]\n",
    "cov_list = [cov_1, cov_2, cov_3, cov_4]\n",
    "\n",
    "N_samples = 900\n",
    "x = GMM_rvs(N_samples,alpha,mu_list,cov_list)\n",
    "\n",
    "plot_2d_GMM(mu_list,cov_list,alpha,samples=x,limits=[-3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 -- GMM pdf\n",
    "\n",
    "implement the function `GMM_pdf` which computes the probability that a given sample is drawn from a GMM. the function receives as input: a sample $x$, the $\\alpha$ parameters as a numpy array, and two lists containing the mean vectors $\\mu$ and covariance matrices $\\Sigma$ of the mixture. The function returns the probability of observing the sample. Make sure the function can process an array of samples as input. \n",
    "\n",
    "hint: In this exercise you can use, if you want, the following scipy function `scipy.stats.multivariate_normal.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_pdf(x,alphas,mu_list,cov_list):\n",
    "    \n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GMM_rvs(3,alpha,mu_list,cov_list)\n",
    "p = GMM_pdf(x,alpha,mu_list,cov_list)\n",
    "print(\"probability of observing the samples: {}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 -- EM algorithm\n",
    "\n",
    "we now want to implement the EM algorithm. The goal of the algorithm is to estimate the parameters $\\alpha$, $\\mu$, and $\\Sigma$ of a GMM starting from a set samples (see course slides for the algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's get the samples\n",
    "x = GMM_rvs(N_samples,alpha,mu_list,cov_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters:\n",
    "K = 4 # components of the gaussian mixture\n",
    "alpha_est =\n",
    "mean_est = \n",
    "cov_est = \n",
    "gamma = np.zeros((N_samples,K)) \n",
    "\n",
    "# online plotting (when you disable this it runs faster)\n",
    "plot = True\n",
    "\n",
    "log_p_iter = []\n",
    "for it in range(0,200):\n",
    "    \n",
    "    # compute log(p(x)) (we use the logarithm as p(x) when N_samples is large gets very small)\n",
    "    log_p_iter.append(np.sum(np.log(GMM_pdf(x,alpha_est,mean_est,cov_est))))\n",
    "\n",
    "    # plotting \n",
    "    if plot:\n",
    "        plot_2d_GMM(mean_est,cov_est,alpha_est,samples=x,limits=[-3,3])\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.close()\n",
    "    \n",
    "    gamma_old = gamma.copy() # this is needed to stop the algorithm when it converges, but only works if gamma is anumpy array\n",
    "    \n",
    "    # E step\n",
    "    #TODO\n",
    "    \n",
    "    # M step\n",
    "    #TODO\n",
    "    \n",
    "    # check convergence    \n",
    "    if np.linalg.norm(gamma-gamma_old) < 1e-4:\n",
    "        print(\"converged at iteration {}\".format(it))\n",
    "        break\n",
    "\n",
    "plot_logp_alpha(log_p_iter,alpha_est)\n",
    "print(\"final value of p(x): {:.3f}\".format(log_p_iter[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the meaning behind visualizing $p(x)$ as a function of the EM iterations, why is it increasing? (note that here we consider $p(x) = p(x|\\mu_\\text{est},\\Sigma_\\text{est},\\alpha_\\text{est})$ to lighten the notation)\n",
    "- In this case you know the value of the components $K=4$, as we know the number of Gaussians in the GMM. What happens when you do not know such number? Try to vary the number K and observe how the algorithm works in these cases. What happens to $p(x)$ when you decrease/increase $K$? why?\n",
    "- Is it possible to make a guess on the number $K$ by looking at the final value of $p(x)$ and to $\\alpha_{\\text{est}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 -- EM for LiDAR data\n",
    "\n",
    "we now want to apply EM and GMM to LiDAR data in order to estimate the vegetation height. We have two LiDAR waveforms which are described by a list of samples. we can think of each sample as the time taken by a photon to travel from the light source, be reflected, and travel back to the sensor. In this case the waveforms are collected by a satellite LiDAR instrument aver a forest region.\n",
    "\n",
    "We are interested in estimating the vegetation height. To do this we need to identify the main first and last peak of the waveform response. The first peak represents the treetop, whereas the last peak comes from the ground level. The ground level peak is usually smaller but rather narrow.\n",
    "\n",
    "To achieve this goal we decide to use GMM. We basically fit a GMM to the data and measure the difference between the mean of the Gaussian distributions centered around those peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we load the data\n",
    "x = np.load(\"samples_waveform_forest_idx_900.npy\")\n",
    "#x = np.load(\"samples_waveform_forest_idx_960.npy\")\n",
    "N_samples = len(x)\n",
    "plot_1d_GMM(samples=x,limits=[0,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can reuse most of the code you wrote above here, just adapt it to the case where the data are one-dimensional\n",
    "\n",
    "# initialize parameters:\n",
    "K =  # components of the gaussian mixture\n",
    "alpha_est = \n",
    "mean_est = \n",
    "var_est = \n",
    "gamma = np.zeros((N_samples,K)) \n",
    "\n",
    "# online plotting (when you disable this it runs faster)\n",
    "plot = False\n",
    "\n",
    "log_p_iter = []\n",
    "for it in range(0,200):\n",
    "    \n",
    "    gamma_old = gamma.copy() # this is needed to stop the algorithm when it converges, but only works if gamma is anumpy array\n",
    "    \n",
    "    # E step\n",
    "    #TODO\n",
    "    \n",
    "    # M step\n",
    "    #ToDO\n",
    "    \n",
    "    # check convergence    \n",
    "    if np.linalg.norm(gamma-gamma_old) < 1e-4:\n",
    "        print(\"converged at iteration {}\".format(it))\n",
    "        break\n",
    "\n",
    "for k in range(0,K):\n",
    "    print(\"component {}: alpha={:.3f} mean={:.3f} std={:.3f}\".format(k,alpha_est[k],mean_est[k],np.sqrt(var_est[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_GMM(mean_est,var_est,alpha_est,samples=x,limits=[0,400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what is, approximately, the best value for $K$?\n",
    "- what is the estimated vegetation height for the two waveforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
