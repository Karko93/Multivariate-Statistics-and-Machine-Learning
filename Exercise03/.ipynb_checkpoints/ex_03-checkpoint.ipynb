{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Statistik und Machine Learning: Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises on this sheet are graded by a maximum of 10(+1) points. You will be asked to implement several functions.\n",
    "\n",
    "Team work is not allowed. Everybody implements his/her own code. Discussing issues with others is fine, sharing code with others is not. \n",
    "\n",
    "If you use any code fragments found on the Internet, make sure you reference them properly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Please do not upload the folder `materials` in your moodle submission.**\n",
    "\n",
    "**Note: you might need to install the cv2 package. you can do so by running `pip install opencv-contrib-python`, otherwise see conda documentation.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 --- Regression (2 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineardemo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 --- Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are given a timeseries (just some observations) that was artificially generated (see file  timeseries.csv). It was obtained combining some known to me and unknown to you polynomial (e.g. y(t)=t2+2 y(t) = t^2 + 2y(t)=t2+2) plus some random noise. Your task is to find the parameters of the polynomial model that was used.\n",
    "\n",
    "Use `pandas.read_csv('./materials/timeseries.csv')` to load the data. In order to visualize data use `plt.plot(timeseries)`.\n",
    "\n",
    "Polynomial fitting is a traditional application of linear regression. Implement function `ii_fit_poly` that fits a polynomial to the input data. The function should take three arguments: the input data (as a column vector), a column vector of target values, and the desired degree of the polynomial (a non-negative integer). Return your estimated\n",
    "model (theta).\n",
    "\n",
    "Also, implement function `ii_apply_poly` that takes two arguments: a model (as returned by your function `ii_fit_poly`) and a column vector of points on which to evaluate the model. Return the estimated values as a column vector.\n",
    "\n",
    "A helper function that extends the feature vector with the polynomial terms (computes $X_\\text{ext}$ from $X$ for a given degree) might be useful, as polynomial fits are also going to be used in later exercises. You can use function test_poly to test your implementation.\n",
    "\n",
    "You have to research which degree works better (in a range $[1, 10]$) and report it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_fit_poly(X, y, degree):\n",
    "    # IMPLEMENT ME: here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_poly():\n",
    "    degrees = [ 1, 3, 5, 9]\n",
    "\n",
    "    timeseries = np.loadtxt('./materials/timeseries.csv')\n",
    "    #print(timeseries.dtype)\n",
    "    x = timeseries[:, 0]\n",
    "    y = timeseries[:, 1]\n",
    "\n",
    "    labels = ['Degree {}'.format(i) for i in degrees]\n",
    "    plt.scatter(x, y, label='Data')\n",
    "\n",
    "    for i in range(len(degrees)):\n",
    "        theta = ii_fit_poly(x, y, degrees[i])\n",
    "        estimated_y = ii_apply_poly(theta, x)\n",
    "        plt.plot(x, estimated_y, label=labels[i])\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Poly fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 --- Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is important for avoiding overfitting on a noisy data input.\n",
    "\n",
    "* Implement function `ii_fit_poly_ridge` that fits a polynomial to the input data using ridge regression. It should take the same arguments as `ii_fit_poly`, except for an additional argument which is the regularization strength $\\lambda$.\n",
    "\n",
    "You can use ridge regression from here `sklearn.linear_model.ridge_regression`.\n",
    "\n",
    "You can use function `test_poly_ridge` to test your implementation.\n",
    "\n",
    "* Try to regularize your model for the Exercise 1 problem.\n",
    "\n",
    "* Experiment how $\\lambda$ influences the quality of your model(to do that you can try to use cross-validation that is not compulsory, but strongly recommended).\n",
    "\n",
    "* Provide the parameters of your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ridge_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def ii_fit_poly_ridge(X, y, degree, lam):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_poly_ridge():\n",
    "    degree = 3\n",
    "    lambdas = [0,10,100,200, 500, 1000]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ts = np.loadtxt('./materials/timeseries.csv')\n",
    "    x = ts[:, 0]\n",
    "    y = ts[:, 1]\n",
    "\n",
    "    labels = ['Lambda {}'.format(i) for i in lambdas]\n",
    "    plt.scatter(x, y, label='Data')\n",
    "\n",
    "    for i in range(len(lambdas)):\n",
    "        theta = ii_fit_poly_ridge(x, y, degree, lambdas[i])\n",
    "        estimated_y = ii_apply_poly(theta, x)\n",
    "        plt.plot(x, estimated_y, label=labels[i])\n",
    "\n",
    "    plt.title('Ridge poly fit')\n",
    "    plt.legend(loc='best')\n",
    "    #plt.savefig('a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poly_ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 --- Face Prediction (2 + 1 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Here we are going to reconstruct right part of human face from the left part of a human face. We will use linear regression to model the right part of the human face as a function of the corresponding left part. Implement function `ii_predict_face` that takes two arguments: a training dataset and a query image with no right part of the face.\n",
    "\n",
    "Training dataset consists of full (left and right part are both there) images of size ($231\\times395$). From those images you have to extract labels (right parts) and construct a dataset in the way that will allow you to apply squared error minimization method. The images in the provided dataset are stacked along the third axis (if you imagine what is the shape of the data - the answer would be [number of images, image length, image width]).\n",
    "\n",
    "You can visualize an image with function `plt.imshow(image)`. The function should return predicted right part with the same shape as the query has (see below).\n",
    "\n",
    "Using the data from the training dataset, learn a linear regression model with the right part of the image as the output value and the left part of the image as input value. You can use `C1=dataset[:, :198, :)` to extract the left part of the images in the dataset.\n",
    "\n",
    "In order to get the shape of the data use function `data.shape`.\n",
    "\n",
    "Your output vector in the linear regression ($Y$ in the intro) should be a $N \\times length \\times width/2$ matrix, your input matrix ($X_\\text{ext}$) should be $N \\times length \\times width/2 + 1$, with ones in the last column and raveled images in the columns left.\n",
    "\n",
    "After you determined the model $\\theta$, estimate the right part of the face using your model. You can use `v.reshape([num_examples, rows, cols])` to convert a column vector representation back into the dataset of images.\n",
    "\n",
    "Estimate the right part of the query by using the previously learned model. Reshape it again to the form of image. You can use script `test_predict_face` to test your implementation.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_predict_face(d, q):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_face():\n",
    "    dataset = []\n",
    "\n",
    "    DATA_FOLDER = './materials/small_dataset/'\n",
    "    \n",
    "    for f in os.listdir(DATA_FOLDER):\n",
    "        if f.endswith('jpg'):\n",
    "            path = os.path.join(DATA_FOLDER, f)\n",
    "            dataset.append(plt.imread(path))\n",
    "        \n",
    "    dataset = np.asarray(dataset, dtype=np.float32)\n",
    "    \n",
    "    d_train, d_val = train_test_split(dataset)\n",
    "    query = d_val[:, :, :d_val.shape[-1] // 2]\n",
    "    prediction = ii_predict_face(d_train, query)\n",
    "    predicted_full = np.concatenate((query, prediction), axis=2)\n",
    "\n",
    "    i = np.random.choice(len(d_val) - 1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    \n",
    "    ax[0, 0].imshow(d_val[i], cmap=plt.cm.gray)\n",
    "    ax[0, 0].set_title('Original')\n",
    "\n",
    "    ax[1, 0].imshow(predicted_full[i], cmap=plt.cm.gray)\n",
    "    ax[1, 0].set_title('Regression result')\n",
    "\n",
    "    ax[0, 1].imshow(d_val[i + 1], cmap=plt.cm.gray)\n",
    "    ax[0, 1].set_title('Original')\n",
    "    \n",
    "    ax[1, 1].imshow(predicted_full[i + 1], cmap=plt.cm.gray)\n",
    "    ax[1, 1].set_title('Regression result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_predict_face()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Regularization is important for avoiding over-fitting on a noisy data input. For the case of images, the model has too much weights and so, regularization might help to improve results significantly.\n",
    "\n",
    "* Implement function `ii_predict_face_reg` that solves the problem from Exercise 3 using ridge regression.\n",
    "It should take the same arguments as `ii_function_predict_face`, except for an additional argument which is the regularization strength $\\lambda$.\n",
    "\n",
    "You can use function `test_predict_face_reg` to test your implementation.\n",
    "\n",
    "* Research, how the choice of $\\lambda$ contributes to the results.\n",
    "* Report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_predict_face_reg(d, q, lam):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_face_reg():\n",
    "    dataset = []\n",
    "\n",
    "    DATA_FOLDER = './materials/small_dataset/'\n",
    "    \n",
    "    for f in os.listdir(DATA_FOLDER):\n",
    "        if f.endswith('jpg'):\n",
    "            path = os.path.join(DATA_FOLDER, f)\n",
    "            dataset.append(plt.imread(path))\n",
    "        \n",
    "    dataset = np.asarray(dataset, dtype=np.float32)\n",
    "    \n",
    "    d_train, d_val = train_test_split(dataset)\n",
    "    query = d_val[:, :, :d_val.shape[-1] // 2]\n",
    "    prediction = ii_predict_face_reg(d_train, query, 1e2)\n",
    "    predicted_full = np.concatenate((query, prediction), axis=2)\n",
    "\n",
    "    i = np.random.choice(len(d_val) - 1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    \n",
    "    ax[0, 0].imshow(d_val[i], cmap=plt.cm.gray)\n",
    "    ax[0, 0].set_title('Original')\n",
    "\n",
    "    ax[1, 0].imshow(predicted_full[i], cmap=plt.cm.gray)\n",
    "    ax[1, 0].set_title('Regression result')\n",
    "\n",
    "    ax[0, 1].imshow(d_val[i + 1], cmap=plt.cm.gray)\n",
    "    ax[0, 1].set_title('Original')\n",
    "    \n",
    "    ax[1, 1].imshow(predicted_full[i + 1], cmap=plt.cm.gray)\n",
    "    ax[1, 1].set_title('Regression result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_face_reg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.* --- Bonus Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve problems from Exercise 3 for dataset of bigger images. For this problem, create copy of `ii_predict_face_reg` add to its names postfix `_big`.\n",
    "\n",
    "The problem here is the size. Analytical method requires expensive operation (matrix inversion, see (Normal Equation)). After the introduction of regularizer, the total size of the problem becomes too big. Estimate the size of matrices for closed form solution of this problem (report it) and you will see that inversion of this matrix is too expensive (it requires O($n^3$) operations).\n",
    "\n",
    "What could be done: iterative methods (see gradient descent (GD) and stochastic average gradient (SAG)) or use extremely powerful PC :).\n",
    "\n",
    "To test your solution use `test_predict_face_reg_big`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def ii_predict_face_reg_big(d, q, lam):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict_face_reg_big():\n",
    "    dataset = []\n",
    "\n",
    "    DATA_FOLDER = './materials/dataset/'\n",
    "    \n",
    "    for f in os.listdir(DATA_FOLDER):\n",
    "        if f.endswith('pgm'):\n",
    "            path = os.path.join(DATA_FOLDER, f)\n",
    "            dataset.append(plt.imread(path))\n",
    "        \n",
    "    dataset = np.asarray(dataset, dtype=np.float32)\n",
    "    \n",
    "    d_train, d_val = train_test_split(dataset)\n",
    "    query = d_val[:, :, :d_val.shape[-1] // 2]\n",
    "    prediction = ii_predict_face_reg_big(d_train, query, 1e2)\n",
    "    predicted_full = np.concatenate((query, prediction), axis=2)\n",
    "\n",
    "    i = np.random.choice(len(d_val) - 1)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "    \n",
    "    ax[0, 0].imshow(d_val[i], cmap=plt.cm.gray)\n",
    "    ax[0, 0].set_title('Original')\n",
    "\n",
    "    ax[1, 0].imshow(predicted_full[i], cmap=plt.cm.gray)\n",
    "    ax[1, 0].set_title('Regression result')\n",
    "\n",
    "    ax[0, 1].imshow(d_val[i + 1], cmap=plt.cm.gray)\n",
    "    ax[0, 1].set_title('Original')\n",
    "    \n",
    "    ax[1, 1].imshow(predicted_full[i + 1], cmap=plt.cm.gray)\n",
    "    ax[1, 1].set_title('Regression result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " test_predict_face_reg_big()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exercise 3 --- Random Forest (3 points)\n",
    " \n",
    " Now we will implement random forest classifier. Let's remember that the idea is to partition the input space\n",
    " and run piece-wise linear regression that maximize the prediction power.\n",
    " \n",
    "Given a dataset $\\mathcal{D} = \\{x, y\\}$ we define a function $f_{\\theta_0,\\theta_1, \\sigma}$. Where we use $\\sigma$ to split the input space from $x$ such that:\n",
    " \n",
    " $$Error_{\\text{before}} = \\sum_{x} (y - \\theta^Tx_i)^2 $$\n",
    " \n",
    " \n",
    " $$Error_{\\text{after}} = \\sum_{x < \\sigma_0} (y - \\theta_0^Tx_i)^2  + \\sum_{x > \\sigma_0} (y - \\theta_1^Tx_i)^2 $$\n",
    "\n",
    "In practice, we can use much more $\\sigma$'s to partition the input space are used, these are usually hierarchical (hence the tree term). For simplicity we will program a regression tree with `nsplits`, this will be randomly defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's simulate some data, and make the traditional train, test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.normal(0, 5, N)\n",
    "f = lambda x:  np.sin(x)* x**2 + 100\n",
    "y = f(x) + 3*np.random.normal(-3, 3, N)\n",
    "x_ =  np.stack((np.ones_like(x),x),axis=-1)\n",
    "# DATASET\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_,y)\n",
    "\n",
    "id_sorted = np.argsort(x_test[:,1])\n",
    "x_test, y_test = x_test[id_sorted] , y_test[id_sorted]\n",
    "\n",
    "plt.scatter(x_train[:,1], y_train, marker='x', label='train')\n",
    "plt.scatter(x_test[:,1], y_test, marker='.', label='test')\n",
    "\n",
    "\n",
    "## GT \n",
    "\n",
    "x_gt = np.linspace(-15,15,10000)\n",
    "\n",
    "plt.plot(x_gt,f(x_gt), 'black', label='GT')\n",
    "\n",
    "plt.xlim(x.min()-1,x.max()+1)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 --- Question\n",
    "\n",
    "Why do we need to add a line of 0's to x?\n",
    "\n",
    "`x_ =  np.stack((np.ones_like(x),x),axis=-1)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(x,y):\n",
    "    theta, sse,_,_ = np.linalg.lstsq(x, y,rcond=None)\n",
    "    mse = sse / x.shape[0]\n",
    "    return theta, mse\n",
    "\n",
    "def predict_tree(x_in, model):\n",
    "    y_out = np.zeros(x_in.shape[0])\n",
    "    for i, x in enumerate(x_in):\n",
    "        for interval, theta in model.items():\n",
    "            if x[1] > interval[0] and x[1] < interval[1]:\n",
    "                y_out[i] = np.dot(x, theta)\n",
    "    return y_out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 --- Complete the `regression_tree` function.\n",
    "\n",
    "First define randomly the sigma parameters (i.e. the thresholds where we will partition x).\n",
    "Hints: Don't forget to add `-np.inf` and `np.inf` to the set of sigma parameters to take into account the complete range. Sorting sigma makes things easier.\n",
    "\n",
    "Ex. if nsplits = 3\n",
    "sigma should be of the form \n",
    "$$\\sigma = (-inf, -0.5 , 0.35, 1.55, inf)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_tree(x,y, nsplits=1, seed=None):\n",
    "    '''\n",
    "    Returns\n",
    "    -------\n",
    "    model_out : dictionary\n",
    "    Each key represents the interval range from input x and\n",
    "    the item is the theta of a linear regression.\n",
    "\n",
    "    '''\n",
    "\n",
    "    model_out = dict()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    sigma = # TODO\n",
    "\n",
    "    for n_ in range(nsplits+1):\n",
    "\n",
    "        # index should be a boolean indicator to filter samples inside the range of interest\n",
    "        \n",
    "        index =  # TODO (you can use np.logical_and to compare several conditions of arrays)\n",
    "        \n",
    "        theta, mse = fit_linear(x_train[index],y_train[index])\n",
    "        model_out[(min_, max_)] = theta\n",
    "    return model_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can train the model \n",
    "model = regression_tree(x_train, y_train,nsplits=4, seed=2)\n",
    "\n",
    "# model keys are the piece-wise intervals and\n",
    "# contains the theta from the linear regression\n",
    "\n",
    "model.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = predict_tree(x_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 --- Question\n",
    "\n",
    "What is the Mean Squared Error on the test set? Which is the best value for `nsplits`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[:,1], y_train, marker='x', label='train')\n",
    "plt.scatter(x_test[:,1], y_test, marker='.', label='test')\n",
    "plt.plot(x_test[:,1],y_pred, 'red', label = 'predicted')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "mse = # TODO\n",
    "\n",
    "plt.title(f'MSE {mse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 --- Ensemble of trees.\n",
    "\n",
    "The real advatange of random trees is to train several random trees and make an ensemble of \n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code to compute an ensemble of predictions. Does it improve the performance? How many random trees would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrees = 10\n",
    "\n",
    "model_list = []\n",
    "for n in range(ntrees):\n",
    "    model_ = # TODO\n",
    "    model_list.append(model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = []\n",
    "\n",
    "for model_ in model_list:\n",
    "    y_pred_ = predict_tree(x_test,model_)\n",
    "    y_predictions.append(y_pred_)\n",
    "    \n",
    "y_predictions = np.stack(y_predictions)\n",
    "y_ensemble = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[:,1], y_train, marker='x', label='train')\n",
    "plt.scatter(x_test[:,1], y_test, marker='.', label='test')\n",
    "plt.plot(x_test[:,1],y_ensemble, 'red', label = 'predicted')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "mse = # TODO\n",
    "\n",
    "plt.title(f'MSE {mse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 --- RANSAC (3 Points)\n",
    "\n",
    "RAndom SAmpling Consensus (RANSAC) is a method for estimating parameters that is particularly effective when dealing with outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as st\n",
    "import cv2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"materials/RANSAC_data.npz\")\n",
    "x = data[\"x\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "plot_linear_model_and_data(data=(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 --- MMSE estimator\n",
    "\n",
    "we know that the data comes from a linear model of the form:\n",
    "$$ y = m x + q + \\epsilon $$\n",
    "where $\\epsilon$ denotes some noise in the observations.\n",
    "\n",
    "write the function `MMSE_estimator` wich receives as input the arrays `x` and `y` and returns the minimum mean square error estimator for the parameters of the linear model `m` and `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMSE_estimator(x,y):\n",
    "    \n",
    "    m = ...\n",
    "    q = ...\n",
    "    \n",
    "    return m,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_mmse, q_mmse = MMSE_estimator(x,y)\n",
    "\n",
    "plot_linear_model_and_data(m=m_mmse,q=q_mmse,data=(x,y))\n",
    "print(\"MMSE estimator: m={:.3f} q={:.3f}\".format(m_mmse,q_mmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 --- RANSAC\n",
    "\n",
    "we now want to use RANSAC to improve the quality of our estimate. first write the function `is_inlier` which receives as input the model parameters, a data point (or a vector of data points, as you prefer), and a threshold $\\delta$; it returns True if the data point is within the tolerance and False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inlier(m,q,x,y,delta):\n",
    "    \"\"\"\n",
    "        m,q: parameters if the linear model\n",
    "        x,y: data point\n",
    "        delta: tolerance for considering a point as inlier or outlier\n",
    "    \"\"\"\n",
    "    \n",
    "    inlier = ...\n",
    "    \n",
    "    return inlier\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write now the RANSAC iterative procedure using the functions `is_inlier` and `MMSE_estimator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_N_inliers = -1\n",
    "best_model = None\n",
    "\n",
    "max_iterations = ... # maximum number of iterations\n",
    "M = ...  # number of subsample of whole dataset extracted in each iteration\n",
    "delta = ... # tolerance for considering a point an inlier\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    \n",
    "    # sample M data points\n",
    "    \n",
    "    # fit the model with randomly sampled points using the MMSE estimator\n",
    "    m, q = \n",
    "    \n",
    "    # count the number of inliers using the estimated model\n",
    "    N_inliers = ...\n",
    "    \n",
    "    # save the model parameters in case they are the best \n",
    "    if #best model so far\n",
    "        best_model = m,q\n",
    "\n",
    "print('best model explains: {}% points'.format(100*best_N_inliers/100.))\n",
    "\n",
    "m_ransac, q_ransac = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_model_and_data(m=m_ransac,q=q_ransac,data=(x,y))\n",
    "print(\"RANSAC estimate: m={:.3f} q={:.3f}\".format(m_ransac,q_ransac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "- which method provides the best results? MMSE or RANSAC?\n",
    "- what is a good value for $M$ and $\\delta$ in this case? motivate your answer.\n",
    "- if we assume the probabilty for each point to be an inlier is equal to 0.7, what should be the value of `max_iterations` in order to hace that at least in one iteration the subsampled dataset is free of outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 --- RANSAC for image stitching \n",
    "\n",
    "we now want to use RANSAC to perform image stitching. Image stitching basically consists in stitching together two images which have some overlapping parts.\n",
    "\n",
    "Of course in order to do this one image must be transformed (translated, rotated...) so that the part of the scene that appears in both images is \"aligned\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1,img2 = load_and_show_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to find the correspondences within the two images, which means find the pixels in the two images which correspond to the same spot in the 3D scene. This part is already done and it is provided to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1,kp2 = load_correspondences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image it is possible to see the corresponedences that are available in the images. The goal is to use these correspondences to estimate how the image 2 needs to be transformed in order to be stitched to image 1. The vectors `kp1` and `kp2` are arrays of size $N\\times 2$, and contain the pixel coordinates of the correspondences for image 1 and image 2.\n",
    "\n",
    "You are provided with the function `estimate_transformation_matrix` which receives as input two arrays of siye $M \\times 2$ of corespondences and estimates the matrix `H` which can then be used to map each pixel of image 2 to a pixel of image 1.\n",
    "\n",
    "Moreover you are provided with the function `transform_points` which receives as input coordinates of pixels (correspondences) in image 2 and maps them to image 1 using the transformation matrix `H`\n",
    "\n",
    "write the function `is_inlier_stitching` which receives as input two arrays of coordinates and checks if the coordinates are pairwise within some threshold $\\delta$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inlier_stitching(kp1,kp2_transformed,delta):\n",
    "    \"\"\"\n",
    "        kp1: correspondence point in image 1\n",
    "        kp2_transformed: correspondence point mapped from image 2 to image 1\n",
    "        delta: tolerance for considering a point as inlier or outlier\n",
    "    \"\"\"\n",
    "    \n",
    "    inlier = ...\n",
    "\n",
    "    return inlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complete the following RANSAC method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_N_inliers = -1\n",
    "best_model = None\n",
    "\n",
    "max_iterations = ... # maximum number of iterations\n",
    "M = ...  # number of subsample of whole dataset extracted in each iteration\n",
    "delta = ... # tolerance for considering a point an inlier\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    \n",
    "    # sample M correspondences from kp1 and kp2\n",
    "    kp1_sample = ...\n",
    "    kp2_sample = ...\n",
    "    \n",
    "    # fit the model with randomly sampled correspondences\n",
    "    H = estimate_transformation_matrix(kp1_sample,kp2_sample)\n",
    "    \n",
    "    # transforms all correspondences from image 2 to image 1\n",
    "    kp2_transformed = transform_points(kp2,H)\n",
    "    \n",
    "    # now check how many points of kp2_transformed are close to the corresponding point kp_1\n",
    "    # use:\n",
    "    # is_inlier_stitching(kp2_transformed,kp1,delta)\n",
    "    N_inliers = ...\n",
    "        \n",
    "    # save the model parameters in case they are the best \n",
    "    if #best model so far\n",
    "        best_model = H\n",
    "    \n",
    "print('best model explains: {:.3f}% points'.format(100 * best_N_inliers/kp1.shape[0]))\n",
    "\n",
    "H = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,mask = blending(img1,img2,H)\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(img[:,:,[2,1,0]])\n",
    "plt.show()\n",
    "\n",
    "_ = cv2.imwrite('stitching.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- considering that the transformation matrix $H$ has 8 degrees of freedom, what is a good value of $M$ in this case?\n",
    "- How do you empirically select in this case the value of $\\delta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
