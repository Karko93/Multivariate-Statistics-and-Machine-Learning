{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Statistik und Machine Learning: Assignment 6\n",
    "In this exercise you will learn how to use neural networs in simulated and real datasets.\n",
    "\n",
    "Please note it is important to answer questions when they are given, as we will reduce points in case no specific answer is given.\n",
    "\n",
    "For this we will use the [Pytorch](https://pytorch.org) library. You can install pytorch using`conda` environment or `pip` (see pytorch website).\n",
    "you will also need `conda install -c pytorch torchvision`\n",
    "\n",
    "__Team work is not allowed__. Everybody implements his/her own code. Discussing issues with others is fine, sharing code and/or answers with others is __not__. If you use any code fragments found on the Internet, make sure you reference them properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1, 1pt\n",
    "\n",
    "In this first exercise we are going to get an idea on how simple neural networks\n",
    "behave for simple classification tasks. We’ll make use of two different javascript\n",
    "based neural network to train and modify an existing model. Let’s start with a very\n",
    "simple one, a simple neural network with only fully connected layers and without\n",
    "convolutions. Open a web browser on the page http://playground.tensorflow.org/ and start playing with the interface. For each dataset set the noise level to 30. Click play on the top-left to start training. To assess the quality of the network look at the values train and test loss at the top right. Tip: Sometimes it’s easier\n",
    "to see the result if you discretise the graph.\n",
    "\n",
    "\n",
    "1. For the spiral dataset try to design the best architecture you can to solve the problem.\n",
    "\n",
    "To get the point in this task you need to add a screenshot to the submission zip file with the best architecture and answer the following:\n",
    "\n",
    "- Now try to find an architecture that is as small as possible (low number of layers, low number of neurons), With a smaller architecture, can you keep a similar performance? You can support your answer with the test-loss numbers from the test set. \n",
    "- In the spiral dataset, do you observe any advantages of having a deep network (more layers) than a wider network (more neurons per layer)? Which one performs better in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "#### Yes, the smallest architecture with the best performance I could found was 3 hidden layers and with (8 neurons,8 neurons, 1 neuron) --> the criteria for the best performance was the robustness of this noisy data (noise = 30) in aspect to the test loss. the test loss was more or less constant around ~0.02 (which was also the smallest) with different regenerated data\n",
    "#### Deeper Networks seem to converge faster and tend to less overdit but on my opinion wider networks work better in this case because the test loss seems to be lower in general in this situation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The following part is optional, but fun!__ Open this page https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html and try to tweak all the parameters you can to obtain the best network (the highest validation accuracy).\n",
    "Scroll down the page and peek inside the network and have a look at different\n",
    "activation and weight as the network gets trained (click pause to make it freeze).\n",
    "Check how the prediction works on the test set in the last part of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 Backpropagation, 3 pts\n",
    "\n",
    "As you saw in the lecture neural networks are just a stack of simple functions, interlayered with non-linearity functions to model complex problems. Everything must be differentiable to use convex optimizers. For any of such optmizers, we need to compute gradients from the output of the network with respect to the input. Which can be easily computed using the chain rule. \n",
    "\n",
    "Let our neural network be defined by the function $f(x) = h_1(h_2(h_3(x)))$.\n",
    "\n",
    "Using the chain rule we can compute \n",
    "\n",
    "\n",
    "$$ \\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial h_1}{\\partial h_2} \\dfrac{\\partial h_2}{\\partial x} $$\n",
    "\n",
    "$$ \\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial h_1}{\\partial h_2} \\dfrac{\\partial h_2}{\\partial h_3}  \\dfrac{\\partial h_3}{\\partial x}$$\n",
    "\n",
    "\n",
    "let $h_1$ and $h_3$ be linear layer of the form $h_1(x) = w_1*x$ and $h_3(x) = 2*w_3*x+b_3+1$. For simplicity we will use an identity instead of a nonlinearity like ReLU or tanh.\n",
    "\n",
    "\n",
    "#### Ex.3.1 Write down the partial derivatives of $h_1$ and $h_3$.\n",
    "\n",
    "This should be done with respect to the input and each of the parameters ($x, w_1, w_3, b_3$). Here we ask you for the formula not the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\dfrac{\\partial h_1}{\\partial x} = w_1 $$\n",
    "$$ \\dfrac{\\partial h_1}{\\partial w_1} = x $$\n",
    "$$ \\dfrac{\\partial h_3}{\\partial x} = 2 \\cdot w_3 $$\n",
    "$$ \\dfrac{\\partial h_3}{\\partial w_3} = 2 \\cdot x $$\n",
    "$$ \\dfrac{\\partial h_3}{\\partial b_3} = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such linear layers are already implemented in most common frameworks. And have automated gradient computation. In Pytorch this is called [Autograd](https://pytorch.org/docs/stable/autograd.html). In pytorch a linear function is defined in the `torch.nn.Linear` class.\n",
    "\n",
    "Which means all the derivatives $\\dfrac{\\partial h_1}{\\partial x}$ and $\\dfrac{\\partial h_3}{\\partial x}$ are computed automatically with autograd.\n",
    "\n",
    "The following code is an example implementation of a Linear function ([see source](https://pytorch.org/docs/stable/notes/extending.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inherit from Function\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            # df/dx = w\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            # df/dw = x\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            # df/fb = 1\n",
    "            grad_bias = grad_output.sum(0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return grad_input, grad_weight, grad_bias\n",
    "        \n",
    "        \n",
    "        \n",
    "linear = LinearFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "in_dim = 4\n",
    "out_dim = 3\n",
    "\n",
    "input = (torch.randn(batch,in_dim,dtype=torch.double,requires_grad=True), # input data\n",
    "         torch.randn(out_dim,in_dim,dtype=torch.double,requires_grad=True),\n",
    "         torch.randn(out_dim,dtype=torch.double,requires_grad=True), # bias\n",
    "        )\n",
    "test = torch.autograd.gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex.2.2 Implement $h_2(x)$ in pytorch.\n",
    "\n",
    "\n",
    "Stacking linear functions with non-linearities creates powerful networks to tackle complex tasks. Some times however more complicaded functions are most fitted to certain settings (e.g. physical constrains, optimization constrains, etc.). Which is why we may need to implement a more complex function in a single layer.\n",
    "\n",
    "Let $h_2$ has the form:\n",
    "\n",
    "$$h_2(x) =  w_{21}*x + w_{22}*x^2 +exp(w_{23}*x)+b_2$$\n",
    "\n",
    "\n",
    "Use the MyFunction class skeleton below. You need to:\n",
    "- write the forward pass (i.e. feed the inputs through the function using the function's parameters)\n",
    "- write your own implementation of the backward pass (i.e. compute $\\dfrac{\\partial h_2}{\\partial x}$ and return the product with the previous gradient. In our example the `grad_output` would be equivalent to $\\dfrac{\\partial h_1}{\\partial h_2}$ and the function should return the product of `grad_output` and the gradients with respect to each of the inputs \n",
    "$(\\dfrac{\\partial h_1}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial x},\n",
    "  \\dfrac{\\partial h_1}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial w_{21}},\n",
    "  \\dfrac{\\partial h_1}{\\partial h_2}\\dfrac{\\partial h_2}{\\partial w_{22}},...)$. \n",
    "\n",
    "Note that you need to return gradients with respect to each of the inputs involved in $h_2$ in the same order the were recieved in the forward pass ($x, w_{21},w_{22},w_{23},b_{21}$).\n",
    "\n",
    "For simplicity, asume that the input is in $\\mathbb{R}^1$ and the function only outputs a single scalar. It must however be able to recieve batches of samples with the shape (batch, 1).\n",
    "\n",
    "\n",
    "Hints:\n",
    "\n",
    "- First write down the formulas of each of the partial derivatives, and then implement them. This should facilitate the writing of the code.\n",
    "- `tensor_x.t()` denotes transpose of tensor_x\n",
    "- for a matrix multiplication between x and y use `x.mm(y)`.\n",
    "- for basic operations you can use:  `tensor_a.sum(tensor_b)` ,  `tensor_a.mul(tensor_b)`,`tensor_a.pow(2)`, `torch.exp(x)` ,  etc. ([see more](https://pytorch.org/docs/stable/tensors.html))\n",
    "- it is important that the function returns pytorch tensors types.\n",
    "- in case you need to create new tensors, make sure they are of type `double`, this is necessarily to make the gradcheck function work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, w21, w22=None, w23=None, b2=None):\n",
    "        ctx.save_for_backward(input, w21,w22,w23,b2)\n",
    "        \n",
    "        output = input.mm(w21)\n",
    "        if w22 is not None:\n",
    "            output += w22*input.pow(2)##\n",
    "        if w23 is not None:\n",
    "            output += torch.exp(w23*input)##\n",
    "        if b2 is not None:\n",
    "            output += b2##\n",
    "                 \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, w21, w22, w23, b2 = ctx.saved_tensors\n",
    "\n",
    "        \n",
    "        grad_input = grad_w21 = grad_w22 = grad_w23 = grad_b2 = None\n",
    "        print(grad_output.shape, input.shape)\n",
    "        grad_input = grad_output.mm(w21)\n",
    "        grad_w21 = grad_output.t().mm(input)\n",
    "        \n",
    "        #print(w22.shape)\n",
    "        if w22 is not None:\n",
    "            grad_input += grad_output.mul(2*input.mm(w22))\n",
    "        if w23 is not None:\n",
    "            grad_input += grad_output.mul(torch.exp(input.mm(w23))*w23)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if w22 is not None and ctx.needs_input_grad[2]:\n",
    "            grad_w22 = grad_output.t().mm(input.pow(2))\n",
    "            \n",
    "        if w23 is not None and ctx.needs_input_grad[3]:\n",
    "            grad_w23 = grad_output.t().mm(input.mul(torch.exp(input.mul(w23))))\n",
    "        if b2 is not None and ctx.needs_input_grad[4]:\n",
    "            grad_b2 = grad_output.sum(0)\n",
    "        \n",
    "        #print(grad_input.shape,grad_w21.shape, grad_w22.shape, grad_w23.shape, grad_b2.shape )\n",
    "        \n",
    "        return grad_input, grad_w21, grad_w22, grad_w23, grad_b2\n",
    "myfunction = MyFunction.apply   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code below you can test your implementation `gradcheck`performs a numerical computation of the gradient to verify that the analytical solution we wrote is correct.\n",
    "\n",
    "You omit some parameters to test the gradients one by one. For example: (input, w21) will compute and check only the gradients where w21 is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 3 #2\n",
    "d_in = 1\n",
    "d_out = 1\n",
    "input_test = (torch.randn(batch,d_in,dtype=torch.double,requires_grad=True), # input\n",
    "         torch.randn(d_out,d_in,dtype=torch.double,requires_grad=True), # w21\n",
    "         torch.randn(d_out,d_in,dtype=torch.double,requires_grad=False), # w22\n",
    "         torch.randn(d_out,d_in,dtype=torch.double,requires_grad=True), # w23\n",
    "         torch.randn(d_out,dtype=torch.double,requires_grad=True) # b21\n",
    "             )\n",
    "\n",
    "res = torch.autograd.gradcheck(myfunction, input_test, raise_exception=False)\n",
    "print(res) # res should be True if the gradients are correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of implentation of a pytorch module using the function we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2(nn.Module):\n",
    "    def __init__(self, input_features, output_features, is_example=False):\n",
    "        # input_features define the dimension of the input\n",
    "        # output_features defines the dimensions of the output\n",
    "        \n",
    "        super(H2, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        \n",
    "        \n",
    "        self.w21 = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        self.w21.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "        if is_example:\n",
    "            self.w22 = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "            self.w23 = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "            self.b2 = nn.Parameter(torch.Tensor(output_features))\n",
    "        \n",
    "            self.w22.data.uniform_(-0.1, 0.1)\n",
    "            self.w23.data.uniform_(-0.1, 0.1)\n",
    "            self.b2.data.uniform_(-0.1, 0.1)\n",
    "        else:\n",
    "            self.w22 = None\n",
    "            self.w23 = None\n",
    "            self.b2 = None\n",
    "            \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        output = myfunction(input, self.w21,self.w22,self.w23, self.b2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = H2(input_features=1,output_features=1, is_example=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3,1)\n",
    "x_out = h2(x)\n",
    "\n",
    "dummy_loss = x_out.sum()\n",
    "dummy_loss.backward()\n",
    "print(dummy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3, 3pts\n",
    "\n",
    "In this exercise you have to train a new Convolutional Neural Network from scratch for the classification of images. The aim is to achieve a high score (98%-99% accuracy on the test set) on the MNIST dataset http://yann.lecun.com/exdb/mnist/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data as datatorch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "##if 'ipykernel' in sys.modules: <--- this statement causes an error\n",
    "    ##import tqdm as tqdm_lib\n",
    "    ##tqdm = tqdm_lib.notebook.tqdm \n",
    "##else:\n",
    "    ##from tqdm import tqdm_notebook as tqdm\n",
    "    \n",
    "def plot_9_digits(MNIST_dataset):\n",
    "    # plot the first 9 training images in MNIST\n",
    "    fig, ax = plt.subplots(3, 3, figsize = (10, 10))\n",
    "    fig.suptitle('First 9 images of MNIST')\n",
    "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
    "    for x, y in [(i, j) for i in range(3) for j in range(3)]:\n",
    "        ax[x, y].axis('off')\n",
    "        ax[x, y].imshow(MNIST_dataset[x + y * 3][0].squeeze(), cmap = 'inferno')\n",
    "        ax[x, y].set_title(MNIST_dataset[x + y * 3][1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the training and test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_train_dataset = datasets.MNIST('data', train=True, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                           transforms.Resize([32, 32]),\n",
    "                           transforms.ToTensor()\n",
    "                           ]))\n",
    "\n",
    "MNIST_test_dataset = datasets.MNIST('data', train=False, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                           transforms.Resize([32, 32]),\n",
    "                           transforms.ToTensor()\n",
    "                           ]))\n",
    "\n",
    "\n",
    "print(\"N train samples: {}\".format(len(MNIST_train_dataset)))\n",
    "print(\"N test samples: {}\".format(len(MNIST_test_dataset)))\n",
    "\n",
    "plot_9_digits(MNIST_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your network using `nn.sequential`. With `nn.sequential` you can simply list sequentially all the layers that form your neural network.\n",
    "\n",
    "hints:\n",
    "- the equivalent of numpy arrays are called tensors in pytorch\n",
    "- pytorch processes data in batches, data have ususally 4 dimensions: [number of samples in the batch] x [number of features] x [spatial dimension] x [spatial dimension 2]\n",
    "- usually the network is composed by a series of blocks each made of a sequence of: one linear layer (fully connected or convolutional layer) followed by a non-linearity (relu, tanh or sigmoid) followed by a pooling layer (average pooling or max pooling). For the MNIST dataset a set of 2-3 of such blocks should be enough.\n",
    "- last layer should be a linear layer that outputs a tensor of dimension [number of samples in the batch] x [number of classes] \n",
    "- `nn.Flatten()` might be a useful module. it transforms a tensor of shape $[N,F, D_1, D_2]$ in one of dimension $[N,F\\cdot D_1 \\cdot D_2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 10\n",
    "input_channels = 1\n",
    "input_spatial_dim = 32\n",
    "\n",
    "\n",
    "model = nn.Sequential(#nn.Linear(32,512, bias = False),\n",
    "                        #nn.ReLU(),\n",
    "                        nn.Flatten(),\n",
    "                        \n",
    "                      nn.Linear(1024, number_of_classes, bias=False),\n",
    "                        \n",
    "                    )\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model =model.to(device)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "logstep = int(1000 // batch_size)\n",
    "\n",
    "train_loader = datatorch.DataLoader(MNIST_train_dataset,batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "test_loader = datatorch.DataLoader(MNIST_test_dataset,batch_size=batch_size, shuffle=False,drop_last=True)\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "# optimizer: you can use torch.optim.SGD, torch.optim.Adam  or any other provided in pytorch lib\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loss_vec = []\n",
    "training_accuracy_vec = []\n",
    "model.train()\n",
    "for e in tqdm(range(0,epochs)):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr/10**e) ## optimiser with converging learningrate\n",
    "    print(e)\n",
    "    training_loss = 0.\n",
    "    training_accuracy = 0.\n",
    "    with tqdm(train_loader,leave=False) as tnr:\n",
    "        tnr.set_postfix(training_loss= np.nan,training_accuracy=np.nan)\n",
    "        for n,(x,y) in enumerate(tnr):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() #  always call this function at the beginning of each step. it basically \"cleans\" the computational graph\n",
    "            \n",
    "            y_pred =  model(x)# compute the prediction using the model\n",
    "            \n",
    "            loss = criterion(y_pred, y) # compute the loss using the batch labels, the predictions and the criterion\n",
    "            \n",
    "            # tell pytorch to compute the gradients wrt the loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # tell pytorch optimizer to make a step using the newly computed gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update stats\n",
    "            training_loss += loss.item()\n",
    "            y_pred_idx = torch.max(y_pred.detach().cpu(),dim=1)[1]\n",
    "            training_accuracy += torch.mean((y_pred_idx == y.cpu()).float()).item()\n",
    "            if (n+1) % logstep == 0:\n",
    "                tnr.set_postfix(training_loss=training_loss/logstep,training_accuracy=training_accuracy/logstep) \n",
    "                training_loss_vec.append(training_loss/logstep)\n",
    "                training_accuracy_vec.append(training_accuracy/logstep)\n",
    "                training_loss, training_accuracy = 0.,0.\n",
    "                \n",
    "\n",
    "plt.plot(logstep*np.arange(1,1+len(training_loss_vec)),np.array(training_loss_vec))\n",
    "plt.ylabel(\"loss criterion\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.show()\n",
    "plt.plot(logstep*np.arange(1,1+len(training_accuracy_vec)),np.array(training_accuracy_vec))\n",
    "plt.ylabel(\"training accuracy\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.show()\n",
    "\n",
    "test_accuracy = 0.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (x,y) in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "                \n",
    "        y_pred = model(x)\n",
    "\n",
    "        y_pred_idx = torch.max(y_pred.detach().cpu(),dim=1)[1]\n",
    "        test_accuracy += torch.mean((y_pred_idx == y.cpu()).float())\n",
    "\n",
    "    test_accuracy = test_accuracy/len(test_loader)\n",
    "        \n",
    "print(\"test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `optimizer` will take care of performing the gradient descent for you, the parameter `lr` modifies the learning rate.\n",
    "- choose a loss to be used for a classification task (see lecture slides and use only [pytorch loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- change the architecture of the network model and the learning rate and observe how the learning curves and performance change.\n",
    "- which non-linearity did you pick? do you notice big performance variation among them? (relu, sigmoid, tanh, leakyRelu...)\n",
    "- How many hidden layers did your best model have? what was the feature dimension of the hidden layers?\n",
    "- what accuracy on the test dataset can you achieve if you use a single linear fully connected layer on the MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer here\n",
    "#### 2.  I picked the ReLU as a non-linearity function. Yes, there are big different up to 10% accuracy between the non-linear functions\n",
    "#### 3.  2 hidden Layers with (32,512) , (16384,2)\n",
    "#### 4.  I get around 92.5% test accuracy with a single linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (3+2 pts)\n",
    "\n",
    "There is a competition [https://www.kaggle.com/c/dogs-vs-cats-mvml-2020/overview](https://www.kaggle.com/c/dogs-vs-cats-mvml-2020/overview). You should create the model, train it, and upload the predictions on kaggle to get the test score on the test set. The first 3 places on the scoreboard will get 2 extra points, between the 4th and 10th place you will get 1 extra point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## added\n",
    "import os\n",
    "import cv2\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms ### added\n",
    "import torch.utils.data as datatorch ### added\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'cats_vs_dogs/cats_vs_dogs'\n",
    "test_dir = 'test/test'\n",
    "train_files = os.listdir(train_dir)\n",
    "test_files = os.listdir(test_dir)\n",
    "\n",
    "class CatDogDataset(Dataset):\n",
    "    def __init__(self, file_list, dir, mode='train', transform = None):\n",
    "        self.file_list = file_list\n",
    "        self.dir = dir\n",
    "        self.mode= mode\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.dir, self.file_list[idx]))\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            if 'dog' in self.file_list[idx]:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.mode == 'train':\n",
    "            img = img.numpy()\n",
    "            return img.astype('float32'), label\n",
    "        else:\n",
    "            img = img.numpy()\n",
    "            return img.astype('float32'), int(self.file_list[idx][:-4])\n",
    "        \n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CatDogDataset(train_files, train_dir, transform = data_transform)\n",
    "test_dataset = CatDogDataset(test_files, test_dir, mode=\"test\" ,transform = data_transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cat.0.jpg\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 8.00 GiB total capacity; 5.90 GiB already allocated; 32.25 MiB free; 29.88 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-805cac27f34b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# compute predictions using model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;31m# compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m    140\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                             self.return_indices)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m     return torch.max_pool2d(\n\u001b[1;32m--> 487\u001b[1;33m         input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[0m\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m max_pool2d = boolean_dispatch(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 8.00 GiB total capacity; 5.90 GiB already allocated; 32.25 MiB free; 29.88 MiB cached)"
     ]
    }
   ],
   "source": [
    "##### first download the dataset and unzipped the image folders ## done\n",
    "\n",
    "# set parameters (learning rate, batch size, number of epochs...)\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "logstep = int(1000 // batch_size)\n",
    "num_workers = 0\n",
    "\n",
    "# create dataloader for training dataset\n",
    "train_loader = datatorch.DataLoader(dataset=train_dataset, \n",
    "                         shuffle=True, \n",
    "                         batch_size=batch_size)\n",
    "\n",
    "\n",
    "# create network model\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=256, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(5,5)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(),\n",
    "            torch.nn.Linear(288, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 2),\n",
    "            torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "model =model.to(device)\n",
    "# create optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500,1000,1500], gamma=0.5)\n",
    "\n",
    "training_loss_vec = []\n",
    "training_accuracy_vec = []\n",
    "\n",
    "# loop over epochs\n",
    "model.train()\n",
    "for e in range(epochs):\n",
    "    training_loss = 0.\n",
    "    training_accuracy = 0.\n",
    "    for n, (x,y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        #print(x.shape)\n",
    "        # call optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions using model\n",
    "        y_pred =  model(x)\n",
    "        # compute loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        # run backward method\n",
    "        loss.backward()\n",
    "        # run optimizer step\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # logging (optional)\n",
    "        training_loss += loss.item()\n",
    "        y_pred_idx = torch.max(y_pred.detach().cpu(),dim=1)[1]\n",
    "        training_accuracy += torch.mean((y_pred_idx == y.cpu()).float()).item()\n",
    "        if (n+1) % logstep == 0: \n",
    "            training_loss_vec.append(training_loss/logstep)\n",
    "            training_accuracy_vec.append(training_accuracy/logstep)\n",
    "            print('training loss: ', training_loss/logstep,'traing_acc: ',training_accuracy/logstep)\n",
    "            training_loss, training_accuracy = 0.,0.\n",
    "   \n",
    "\n",
    "# create dataloader for test dataset\n",
    "# get predictions, save the file and submit it to kaggle (see the kaggle page for info on the submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logstep*np.arange(1,1+len(training_loss_vec)),np.array(training_loss_vec))\n",
    "plt.ylabel(\"loss criterion\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.show()\n",
    "plt.plot(logstep*np.arange(1,1+len(training_accuracy_vec)),np.array(training_accuracy_vec))\n",
    "plt.ylabel(\"training accuracy\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.show()\n",
    "print(max(training_accuracy_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jaeboklee/pytorch-cat-vs-dog\n",
    "testloader = datatorch.DataLoader(test_dataset, batch_size = 1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "fn_list = []\n",
    "pred_list = []\n",
    "i = 0\n",
    "for x, fn in testloader:\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "        output = model(x)\n",
    "        \n",
    "        \n",
    "        pred = torch.argmax(output, dim=1)\n",
    "       \n",
    "        fn_list.append(fn.item())\n",
    "        pred_list += [p.item() for p in pred]\n",
    "        \n",
    "\n",
    "submission = pd.DataFrame({\"id\":fn_list, \"label\":pred_list})\n",
    "submission.to_csv('submission7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 88.8%\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(),\n",
    "            torch.nn.Linear(9216, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 2),\n",
    "            torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs to be tested\n",
    "model = torch.nn.Sequential(\n",
    "       nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(9,9)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(9,9)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(9,9)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3,3)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Linear(64,32,bias = True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.8),\n",
    "        nn.Linear(32,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_accuracy = 0.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for (x,y) in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "                \n",
    "        y_pred = model(x)\n",
    "\n",
    "        y_pred_idx = torch.max(y_pred.detach().cpu(),dim=1)[1]\n",
    "        test_accuracy += torch.mean((y_pred_idx == y.cpu()).float())\n",
    "\n",
    "    test_accuracy = test_accuracy/len(test_loader)\n",
    "        \n",
    "print(\"test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss += loss.item()\n",
    "        y_pred_idx = torch.max(y_pred.detach().cpu(),dim=1)[1]\n",
    "        training_accuracy += torch.mean((y_pred_idx == y.cpu()).float()).item()\n",
    "        if (n+1) % logstep == 0:\n",
    "            tnr.set_postfix(training_loss=training_loss/logstep,training_accuracy=training_accuracy/logstep) \n",
    "            training_loss_vec.append(training_loss/logstep)\n",
    "            training_accuracy_vec.append(training_accuracy/logstep)\n",
    "            training_loss, training_accuracy = 0.,0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "model = torch.nn.Sequential(\n",
    "       nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(9,9)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(9,9)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(9,9)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3,3)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Linear(64,32,bias = True),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.8),\n",
    "        nn.Linear(32,2)\n",
    ")\n",
    "\n",
    "\n",
    "convnet = input_data(shape=[None, IMAGE_SIZE, IMAGE_SIZE, 1], name='input')\n",
    "#Conv Layer 1\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "#Conv Layer 2\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "#Conv Layer 3\n",
    "convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "#Conv Layer 4\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "#Conv Layer 5\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "#Conv Layer 6\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "#Fully Connected Layer with SoftMax as Activation Function\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(5,5), stride=2, padding=1)\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(5,5), stride=2, padding=1)\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=1)\n",
    "        \n",
    "        nn.BatchNorm2d(64)\n",
    "        nn.Dropout(0.5)\n",
    "        \n",
    "        nn.Linear(in_features=64*2*2, out_features=200)\n",
    "        nn.Linear(in_features=200, out_features=20)\n",
    "        nn.Linear(in_features=20, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Flatten(),\n",
    "                                nn.Linear(196608, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 2),\n",
    "                                 nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(num_in_features, hidden_layers, num_out_features):       \n",
    "    classifier = nn.Sequential()    \n",
    "    if hidden_layers == None:        \n",
    "        classifier.add_module('fc0', nn.Linear(num_in_features, 102))    \n",
    "    else:        \n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])        \n",
    "        classifier.add_module('fc0', nn.Linear(num_in_features, hidden_layers[0]))        \n",
    "        classifier.add_module('relu0', nn.ReLU())        \n",
    "        classifier.add_module('drop0', nn.Dropout(.6))        \n",
    "        classifier.add_module('relu1', nn.ReLU())        \n",
    "        classifier.add_module('drop1', nn.Dropout(.5))        \n",
    "    for i, (h1, h2) in enumerate(layer_sizes):            \n",
    "        classifier.add_module('fc'+str(i+1), nn.Linear(h1, h2))            \n",
    "        classifier.add_module('relu'+str(i+1), nn.ReLU())            \n",
    "        classifier.add_module('drop'+str(i+1), nn.Dropout(.5)) \n",
    "        \n",
    "    classifier.add_module('output', nn.Linear(hidden_layers[-1], num_out_features))            \n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(123,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.6),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.5),\n",
    "    \n",
    "    nn.Linear(128, 64),       \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.5),\n",
    "    \n",
    "    nn.Linear(64, 32),       \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.5),\n",
    "    \n",
    "    nn.Linear(32, 16),       \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.5),\n",
    "    \n",
    "    nn.Linear(16,2)\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.1",
    "jupytext_version": "1.2.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
