{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification – Assignment 4\n",
    "\n",
    "The exercises on this sheet are graded by a maximum of 10 points. You will be asked to implement several functions.\n",
    "\n",
    "Team work is not allowed. Everybody implements his/her own code. Discussing issues with others is fine, sharing code with others is not. \n",
    "\n",
    "If you use any code fragments found on the Internet, make sure you reference them properly.\n",
    "\n",
    "Except for Ex. 2 (LDA), we will start using [Scikit-learn](http://scikit-learn.org/stable/), a package that implements most of the machine-learning algorithms we have been using. It is quite usefull to learn how to use it but most importantly we should understand what exactly is the algorithm is doing.\n",
    "\n",
    "Your functions should work when called by the provided test code (functions prefixed with `test` ) which must not be modified. When run, they should produce a plausible output, no warnings, and no unnecessary output. The functions that you write take images as arguments, not image filenames (i.e. do not use imread inside the functions). Your functions should not generate figures/plots themselves, the plots are generated by the test functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise focuses on using classification methods, as\n",
    "implementing most classifiers takes more time than is reasonable in the scope of\n",
    "this assignment. The exception is the exercise about LDA, which is\n",
    "simple enough to be done manually. The rest of the programming exercises are very\n",
    "short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the regression assignment, $Y$ stands for a column vector of \"_target_\" values, that is the $i$-th row of $Y$ contains the desired output for the $i$-th\n",
    "data point. Contrary to regression, the elements of $Y$ are integer values. In\n",
    "most exercises we use binary classifiers, where one class has the label $1$ and\n",
    "the other class has the label $2$. \n",
    "$X$ is a matrix containing the feature values, where the $i$-th row contains\n",
    "the values for the $i$-th data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic data are generated randomly for each run in the\n",
    "\"_toy example_\" test functions, so you can run the functions multiple times to get\n",
    "different outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your answers, comment on the performance of the individual\n",
    "methods when applied on the aerial photo of Graz. Describe also the difference\n",
    "between the results on data seen during training (the upper quarter of the image) and\n",
    "data that are \"_new_\" (the rest of the image). Does a given method work equally\n",
    "well for every class?}\n",
    "You can (and actually need to in order to give a full answer)\n",
    "change the \"_true_\" class in the test functions that use binary\n",
    "classification by changing the __true_class__ variable if you want to see\n",
    "outputs for different classes.\n",
    "\n",
    "Hint: To create slices of training data depending on the label value you can use something like this:`X_1 = X[Y == 1, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 --- Nearest Neighbours (1 Points)\n",
    "\n",
    "Nearest neighbour classification (especially its k-NN variant) is a simple\n",
    "but yet quite powerful classification method which does not make any \n",
    "assumptions about the data model.\n",
    "\n",
    "Using the function [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) k-NN classification is demonstrated\n",
    "in using 2 different datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Toy Dataset\n",
    "\n",
    "Implemented in `demo_knn_toy`. Classes:\n",
    "- 1: blue\n",
    "- 2 : red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY HERE ###########################################################\n",
    "def synthetic_nonlin():\n",
    "    def pol2cart(rho, phi):\n",
    "        x = rho * np.cos(phi)\n",
    "        y = rho * np.sin(phi)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    n_samples = 100\n",
    "    X_1 = np.random.randn(n_samples, 2) / 2\n",
    "    \n",
    "    a, b = pol2cart(np.random.rand(n_samples, 1) * 2 * np.pi, np.random.randn(n_samples, 1) / 3 + 1.5)\n",
    "    X_2 = np.hstack([a, b])\n",
    "    X = np.concatenate([X_1, X_2], axis=0)\n",
    "    Y = np.concatenate([np.ones((n_samples)), 2 * np.ones((n_samples))]) - 1\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def decision_boundaries_vec(x_range, steps, classifier):\n",
    "    l = np.linspace(x_range[0], x_range[1], steps)\n",
    "    x, y = np.meshgrid(l, l)\n",
    "    \n",
    "    features = np.concatenate([x.reshape(-1, 1), y.reshape(-1, 1)], axis=-1)\n",
    "    \n",
    "    if hasattr(classifier, 'predict'):\n",
    "        return x, y, classifier.predict(features).reshape(x.shape)\n",
    "    else:\n",
    "        return x, y, classifier(features).reshape(x.shape)\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toy():\n",
    "    X, Y = synthetic_nonlin()\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(X[Y == 1, 0], X[Y == 1, 1], c='r', marker='*')\n",
    "    plt.scatter(X[Y == 0, 0], X[Y == 0, 1], c='b', marker='o')\n",
    "    plt.title('Toy Data')\n",
    "\n",
    "plot_toy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def demo_knn():\n",
    "    X, Y = synthetic_nonlin()\n",
    "    algos = [\"ball_tree\" ,\"kd_tree\" ,\"brute\",\"auto\"]\n",
    "    metrics = [\"chebyshev\" , \"euclidean\", \"manhattan\",\"minkowski\"]\n",
    "    neighbors = np.arange(2,10)\n",
    "    splits = 5\n",
    "    kf = KFold(n_splits=splits, shuffle = False)\n",
    "    best_k = 1\n",
    "    best_algo = 0\n",
    "    best_metric = 0\n",
    "    best_classifier = 0\n",
    "    best_precision = 0\n",
    "    for neighbor in neighbors:\n",
    "        for algo in algos:\n",
    "            for metric in metrics:\n",
    "                classifier = KNeighborsClassifier(n_neighbors=neighbor , metric=metric,algorithm=algo)\n",
    "                precision = []\n",
    "                for train_index, test_index in kf.split(X):\n",
    "                    X_train, X_test = X[train_index], X[test_index]\n",
    "                    y_train, y_test = Y[train_index], Y[test_index]\n",
    "                    classifier.fit(X_train, y_train)\n",
    "                    y_hat = classifier.predict(X_test)\n",
    "                    precision.append(np.sum(1*(y_hat==y_test))/y_hat.shape[0])\n",
    "                \n",
    "                if np.mean(precision) > best_precision:\n",
    "                    best_precision = np.mean(precision)\n",
    "                    best_model = classifier\n",
    "                    best_k = neighbor\n",
    "                    best_metric =classifier.effective_metric_#metric\n",
    "                    best_algo = algo\n",
    "                    \n",
    "\n",
    "    x_range = np.array([X.min(), X.max()])\n",
    "    x, y, decision_boundaries = decision_boundaries_vec(x_range, 1000, classifier)\n",
    "    ## Validation\n",
    "    print(\"Parameter with the best average precision of \", best_precision, f\" and K-Fold of {splits}\")\n",
    "    print(\"best parameter K: \", best_k)\n",
    "    print(\"best metric (distance):\", best_metric)\n",
    "    print(\"best algorithm (rule)\", best_algo)\n",
    "    k_neighbors = best_k\n",
    "   \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(x, y, decision_boundaries)\n",
    "    plt.scatter(X[Y == 1, 0], X[Y == 1, 1], c='r', marker='*')\n",
    "    plt.scatter(X[Y == 0, 0], X[Y == 0, 1], c='b', marker='o')\n",
    "    plt.title(f'KNN (K={k_neighbors})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the demo functions for different values of parameter $K$,\n",
    "$distance$ and $rule$ and comment the\n",
    "results in your report. Also report the best parameters with explanation\n",
    "why you think that they are the best. Please provide a metric error to support your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### After bruteforcing all variations and calculating the average precission \n",
    "####(right prediction to amount of data with different k-fold sizes)\n",
    "#### best metric (distance): manhattan , best algorithm (rule), best parameter K: varies between 2-4 (so I guess it's 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_knn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Graz aerial photo\n",
    "implemented in `demo_knn_image`.\n",
    " Classes:\n",
    "<div>\n",
    "<img src=\"data/graz_5.tif\" width=\"270\"/>\n",
    "</div>\n",
    "\n",
    "- 1: streets\n",
    "- 2: buildings\n",
    "- 3: vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY HERE ###########################################################\n",
    "def load_graz(scale):\n",
    "    img = cv2.imread('data/graz_5.tif')\n",
    "    gt = cv2.imread('data/gt_graz_5.tif', 0)\n",
    "    gt[gt == 4] = 3\n",
    "    new_scale = tuple((np.array(img.shape) * scale).astype(np.int64))[:-1]\n",
    "\n",
    "    return cv2.resize(img, new_scale), cv2.resize(gt, new_scale)\n",
    "\n",
    "\n",
    "def create_features(varargin):\n",
    "    sy, sx, nc = varargin.shape\n",
    "    return varargin.reshape(sy * sx, nc)\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_knn_image(k,metric,rule):\n",
    "    img, gt = load_graz(0.5)\n",
    "\n",
    "    sy, sx, nc = img.shape\n",
    "\n",
    "    X = create_features(img)\n",
    "\n",
    "    train_columns = sx // 3 ## change from 4 to 2\n",
    "\n",
    "    # change here to make the classification for different classes (1,2 or 3)\n",
    "    true_class = 3\n",
    "\n",
    "    Y = (gt == true_class) + 1\n",
    "\n",
    "    used_train = train_columns * sy\n",
    "\n",
    "    Y = Y.reshape(sy * sx)\n",
    "    \n",
    "    #algos = [\"ball_tree\" ,\"kd_tree\" ,\"brute\",\"auto\"]\n",
    "    #metrics = [\"chebyshev\" , \"euclidean\", \"manhattan\",\"minkowski\"]\n",
    "    #neighbors = np.arange(2,30)\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k, metric=metric, algorithm=rule)\n",
    "    classifier.fit(X[:used_train], Y[:used_train])\n",
    "    X_test, y_test = X[used_train+1:], Y[used_train+1:]\n",
    "    # Here we predict over the complete image \n",
    "    # including training data, which should not be used if measuring the error.\n",
    "    results = classifier.predict(X).reshape(sy,sx)\n",
    "    test_results = classifier.predict(X_test)\n",
    "    precision =  np.sum(1*(test_results == y_test))/y_test.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    axs[0].imshow(results, cmap=plt.cm.gray)\n",
    "    axs[0].set_title(\"Results\")\n",
    "    axs[1].imshow(Y.reshape(sy, sx), cmap=plt.cm.gray)\n",
    "    axs[1].set_title(\"Original\")\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function doesn't finish if I'm bruteforcing it within the function\n",
    "best_precision = 0\n",
    "best_k = 0\n",
    "best_metric = 0\n",
    "best_rule = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [\"ball_tree\" ,\"kd_tree\" ,\"auto\"] # possible rules\n",
    "metrics = [\"chebyshev\" , \"euclidean\", \"manhattan\",\"minkowski\"] # distance metric\n",
    "neighbors = np.arange(20,30) # amount of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for neigh in neighbors: # looping through all options\n",
    "prec = demo_knn_image(k = neigh, metric = \"manhattan\", rule = 'auto')\n",
    "if prec >= best_precision:\n",
    "    best_k = neigh\n",
    "    best_precision = prec\n",
    "    print(best_k)\n",
    "    print(\"best precision: \", best_precision)\n",
    "#print(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_k, best_metric, best_rule, best_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([0.9667902203007204, 0.8679872334564878, 0.8686578961550668])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### After looping through looping through an array of different K's, distance options and algorithms\n",
    "### best Parameter with the highest (reasonable in time) average precision: K= 20, metric = manhattan and\n",
    "###  algorithm = auto --> auto looks for the best algorithm for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the functions for different values of parameter $K$, $distance$ and $rule$ and comment the\n",
    "results in your report. Also report the best parameters with explanation\n",
    "why you think that they are the best. Same as before, you may want to measure the error to support your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 --- LDA (4 Points)\n",
    "\n",
    "LDA classifier can be used to efficiently classify linearly separable data.\n",
    "Implement function `ii_train_lda` that trains an LDA classifier. The\n",
    "input arguments are the training data (one row per sample and one column per\n",
    "feature) and a column vector with the class labels of the samples (0\n",
    "or 1). Return the normal $\\theta$ of the decision boundary and the threshold\n",
    "$t$ between classes (so that if $X\\theta - t > 0$, class $1$ is predicted).\n",
    "Since we use row vectors as features, the formulas are slightly different\n",
    "from those seen in the lecture (but $\\theta$ is still a column vector).\n",
    "\n",
    "$m_1$ and $m_2$ are the means of the input features over all rows\n",
    "where $Y$ is $0$ or $1$ respectively. You can calculate mean and covariance\n",
    "for the classes with corresponding __numpy__ methods.\n",
    "In order to do operations on the rows of $X$ where the corresponding row in $Y$\n",
    "has a certain value (e.g.\\ $1$), you can use the expression `X[Y==1,:]`.\n",
    "Instead of determining $\\theta$ by matrix inversion, which is numerically\n",
    "unstable, use __numpy/scipy__ equation solving capabilities:\n",
    "\n",
    "\n",
    "`normal = np.linalg.lstsq(SW, (m2-m1).T)[0]`\n",
    "\n",
    "\n",
    "where SW is the sum of within-class covariances.\n",
    "The threshold you output is the value of the projection at the point located\n",
    "in the middle between the class means (the (dot) product of the mean of\n",
    "$m_1$ and $m_2$ and the estimated normal).\n",
    "\n",
    "The corresponding `test_lda` is already provided. You can use\n",
    "functions `test_lda` and `test_lda2` to test your implementation and generate\n",
    "results for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_train_lda(X,Y):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: input matrix of size Nxd, where d is the number of features and N the number of samples\n",
    "    Y: label vector of size N. int values 0 or 1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normal: vector of size d\n",
    "    t: boundary in projected space\n",
    "    '''\n",
    "    \n",
    "    #source: https://www.python-course.eu/linear_discriminant_analysis.php\n",
    "    X_zero = X[Y==0,:]\n",
    "    X_one = X[Y==1,:]\n",
    "\n",
    "    mean_zero = np.mean(X_zero, axis = 0)\n",
    "    mean_one = np.mean(X_one, axis = 0)\n",
    "    cov_zero = np.cov(X_zero.T)\n",
    "    cov_one = np.cov(X_one.T)\n",
    "    #cov_zero = np.dot((X_zero-mean_zero).T,(X_zero-mean_zero)) #scatter matrix\n",
    "    #cov_one = np.dot((X_one-mean_one).T,(X_one-mean_one))\n",
    "    SW = cov_zero + cov_one\n",
    "    normal = np.linalg.lstsq(SW, (mean_one-mean_zero).T, rcond = -1)[0]\n",
    "    normal = normal/np.linalg.norm(normal,2)\n",
    "    t = normal.dot((mean_zero - mean_one)*0.5) # This is not the optimal threshold if you ask several other sources\n",
    "    \n",
    "    \n",
    "    return normal, t\n",
    "\n",
    "def ii_test_lda(X, normal, threshold):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: input matrix of size Nxd, where d is the number of features and N the number of samples\n",
    "    normal: vector of size d\n",
    "    t: boundary in projected space\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred: Nx1 predicted class\n",
    "    '''\n",
    "    projected_X = np.matmul(X,normal)\n",
    "    y_pred = 1*(projected_X-threshold > 0)\n",
    "    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY HERE ###########################################################\n",
    "def rotate_base(mat, ang):\n",
    "    R = np.array([\n",
    "        [np.cos(ang), np.sin(ang)],\n",
    "        [-np.sin(ang), np.cos(ang)]\n",
    "    ])\n",
    "    \n",
    "    return R.T.dot(mat).dot(R)\n",
    "\n",
    "def mvarnormal(numelements, Sigma, mu):\n",
    "    R = np.linalg.cholesky(Sigma)\n",
    "    data = np.tile(mu, (numelements, 1))\n",
    "    data = data + np.random.randn(numelements, len(mu)).dot(R)\n",
    "    return data\n",
    "\n",
    "\n",
    "def two_gaussians(samples_per_gaussian, m1, m2, S1, S2):\n",
    "    Y = np.ones((samples_per_gaussian * 2))\n",
    "    Y[-samples_per_gaussian:] = 0\n",
    "    X = np.concatenate([mvarnormal(samples_per_gaussian, S1, m1), mvarnormal(samples_per_gaussian, S2, m2)])\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def synthetic_lin():\n",
    "    S = np.array([[10, 0],\n",
    "                  [0, 5]])\n",
    "    X, Y = two_gaussians(50, [-4, 0], [4, 0], rotate_base(S, np.pi / 4), rotate_base(S, np.pi / 3))\n",
    "    return X, Y\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations as comb\n",
    "\n",
    "def del_first_col(x):\n",
    "    return x[:, 1:].copy()\n",
    "\n",
    "def x2fx(x, model='linear'):\n",
    "    linear = np.c_[np.ones(x.shape[0]), x]\n",
    "    if model == 'linear':\n",
    "        return linear\n",
    "    if model == 'purequadratic':\n",
    "        return np.c_[linear, x**2]\n",
    "    interaction = np.hstack([x[:,i]*x[:,j] for i, j in\n",
    "                                 comb(range(x.shape[1]), 2)]).T\n",
    "    if model == 'interaction':\n",
    "        return np.c_[linear, interaction]\n",
    "    if model == 'quadratic':\n",
    "        return np.c_[linear, interaction, x**2]\n",
    "\n",
    "def test_lda():\n",
    "    x_range = [-10, 10]\n",
    "\n",
    "    X, Y = synthetic_lin()\n",
    "\n",
    "    normal, threshold = ii_train_lda(X, Y)\n",
    "\n",
    "    x, y, decision_boundaries = decision_boundaries_vec(\n",
    "        x_range, 1000, lambda x: ii_test_lda(x, normal, threshold)\n",
    "    )\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    axs[0].contourf(x, y, decision_boundaries)\n",
    "    axs[0].scatter(X[Y == 0, 0], X[Y == 0, 1], c='r', marker='*')\n",
    "    axs[0].scatter(X[Y == 1, 0], X[Y == 1, 1], c='b', marker='o')\n",
    "    axs[0].set_title('LDA')\n",
    "\n",
    "    x_range = [-5, 5]\n",
    "    X, Y = synthetic_nonlin()\n",
    "\n",
    "    normal, threshold = ii_train_lda(del_first_col(x2fx(X, 'quadratic')), Y)\n",
    "\n",
    "    x, y, decision_boundaries = decision_boundaries_vec(\n",
    "        x_range, 1000, lambda x: ii_test_lda(del_first_col(x2fx(x, 'quadratic')), normal, threshold)\n",
    "    )\n",
    "    \n",
    "    axs[1].contourf(x, y, decision_boundaries)\n",
    "    axs[1].scatter(X[Y == 0, 0], X[Y == 0, 1], c='r', marker='*')\n",
    "    axs[1].scatter(X[Y == 1, 0], X[Y == 1, 1], c='b', marker='o')\n",
    "    axs[1].set_title('LDA (lifted)')\n",
    "\n",
    "\n",
    "def test_lda2():\n",
    "    img, gt = load_graz(0.5)\n",
    "\n",
    "    sy, sx, nc = img.shape\n",
    "\n",
    "    X = create_features(img)\n",
    "\n",
    "    train_columns = sx // 4\n",
    "\n",
    "    true_class = 3\n",
    "\n",
    "    Y = gt == true_class\n",
    "\n",
    "    used_train = train_columns * sy\n",
    "\n",
    "    Y = Y.reshape(sy * sx)\n",
    "\n",
    "    normal, threshold = ii_train_lda(X[:used_train, :], Y[:used_train])\n",
    "\n",
    "    results = ii_test_lda(X, normal, threshold)\n",
    "    print(\"precision graz: \",np.sum((1*Y == results))/Y.shape)\n",
    "    results = results.reshape(sy, sx)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    axs[0].imshow(results, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Results')\n",
    "    axs[1].imshow(Y.reshape(sy, sx), cmap=plt.cm.gray)\n",
    "    axs[1].set_title('Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lda2()\n",
    "test_lda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 --- Logistic Regression (2 Points)\n",
    "\n",
    "Logistic regression is a probabilistically principled classification method.\n",
    "You are provided with the breast cancer dataset. You task is to implement function `ii_train_logress`\n",
    "that uses __sciki-learn__ class `sklearn.linear_model.LogisticRegression` to fit a logistic regression model to the data. It should take\n",
    "the same input arguments as train_lda and return the trained model. Also implement function `ii_test_logress`\n",
    "that takes the trained model and input\n",
    "features (in the common format) and returns the output of the logistic\n",
    "regression model. Make sure you cget through the documentation for [logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "You can use functions `test_logress` and `test_logress2` to test your\n",
    "implementation.\n",
    "\n",
    "Implement the function `classify_breast_cancer_with_logress` which using the functions `ii_train_logress` and\n",
    "`ii_test_logress` can distinguish benign and malignant tumor. Use 75% of the dataset for training.\n",
    "\n",
    "Note, that there are missing values in the dataset. You have to either:\n",
    "* remove such examples\n",
    "* interpolate missing values\n",
    "\n",
    "in order to let python load the data for you with `pandas.read_csv('data/data.csv')`.\n",
    "\n",
    "You can find description of the dataset in file _description.txt_.\n",
    "Report how well logistic regression works for the breast cancer prediction (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_train_logress(X,Y):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: input matrix of size Nxd, where d is the number of features and N the number of samples\n",
    "    Y: label vector of size N. int values 0 or 1.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model: trained model of class sklearn.linear_model.©\n",
    "    '''\n",
    "    model = LogisticRegression(max_iter = 10000)\n",
    "    model.fit(X,Y)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def ii_test_logress(model, X):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: trained model of class sklearn.linear_model.\n",
    "    X: input matrix of size Nxd, where d is the number of features and N the number of samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: output of the logistic regression model\n",
    "    '''    \n",
    "    \n",
    "    out = model.predict(X)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logres():\n",
    "    x_range = [-10, 10]\n",
    "    X, Y = synthetic_lin()\n",
    "    Y = Y * 2 - 1\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.75)\n",
    "    \n",
    "    model = ii_train_logress(X_train, y_train)\n",
    "\n",
    "    x, y, decision_boundaries = decision_boundaries_vec(\n",
    "        x_range, 1000, lambda x: (ii_test_logress(model, x) > 0.5) * 2 - 1\n",
    "    )\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    axs[0].contourf(x, y, decision_boundaries)\n",
    "    axs[0].scatter(X[Y == -1, 0], X[Y == -1, 1], c='r', marker='*')\n",
    "    axs[0].scatter(X[Y == 1, 0], X[Y == 1, 1], c='b', marker='o')\n",
    "    axs[0].set_title('Logistic Regression') \n",
    "\n",
    "    x_range = [-2.5, 2.5]\n",
    "\n",
    "    X, Y = synthetic_nonlin()\n",
    "    Y = Y * 2 - 1\n",
    "\n",
    "    model = ii_train_logress(del_first_col(x2fx(X,'quadratic')), Y)\n",
    "\n",
    "    x, y, decision_boundaries = decision_boundaries_vec(\n",
    "        x_range, 1000,\n",
    "        lambda x: 1 + (0.5 < ii_test_logress(model, del_first_col(x2fx(x, 'quadratic'))))\n",
    "    )\n",
    "\n",
    "    axs[1].contourf(x, y, decision_boundaries)\n",
    "    axs[1].scatter(X[Y == -1, 0], X[Y == -1, 1], c='r', marker='*')\n",
    "    axs[1].scatter(X[Y == 1, 0], X[Y == 1, 1], c='b', marker='o')\n",
    "    axs[1].set_title('Logistic Regression (lifted)') \n",
    "\n",
    "\n",
    "def test_logres2():\n",
    "    ## ask if there are already splitting the data\n",
    "    img, gt = load_graz(0.5)\n",
    "\n",
    "    sy, sx, nc = img.shape\n",
    "\n",
    "    X = create_features(img)\n",
    "\n",
    "    train_columns = sx // 4\n",
    "\n",
    "    true_class = 1\n",
    "\n",
    "    Y = (gt==true_class) * 2 - 1\n",
    "\n",
    "    used_train = train_columns * sy\n",
    "\n",
    "    Y = Y.reshape(sy * sx, 1)\n",
    "\n",
    "    model  = ii_train_logress(X[:used_train, :], Y[:used_train, :])\n",
    "    results = ii_test_logress(model, X)\n",
    "    results = results.reshape(sy, sx) > 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    axs[0].imshow(results, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Results')\n",
    "    axs[1].imshow(Y.reshape(sy, sx), cmap=plt.cm.gray)\n",
    "    axs[1].set_title('Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_breast_cancer_with_logress():\n",
    "    data = pd.read_csv(\"data/data.csv\", header = None)\n",
    "    data = data[np.all(data!=\"?\", axis=1)]\n",
    "    data = np.array(data)\n",
    "    X = data[:,1:-1]\n",
    "    Y = data[:,data.shape[1]-1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.75)\n",
    "    model = ii_train_logress(X_train, y_train)\n",
    "    y_hat = ii_test_logress(model, X_test)\n",
    "    accuracy = accuracy_score(y_hat,y_test)\n",
    "    score = f1_score(y_test, y_hat,pos_label=2, average='binary', sample_weight=None, zero_division='warn')\n",
    "    print(\"Accuracy of the Logistic Regression Model is: \", accuracy)\n",
    "    #print(\"F1 Score of the Logistic Regression Model is: \", score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression Model is:  0.9707602339181286\n"
     ]
    }
   ],
   "source": [
    "classify_breast_cancer_with_logress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 --- DecisionTree Classification (1 Points)\n",
    "\n",
    "Classification using random trees and forests is currently very popular\n",
    "thanks to its efficiency and flexibility.\n",
    "Implement function `ii_train_tree` that uses sklearn to train a decision tree. The function should use the same\n",
    "input arguments as the previous training functions, but note that the tree\n",
    "classifier can handle more than two classes, so this time the input is not\n",
    "constrained to -1 or 1 (you do not need to handle this, this is done\n",
    "automatically by the classifier). The function returns the trained decision\n",
    "tree. Also implement function `ii_test_tree` that takes the trained\n",
    "decision tree and input features (in the common format) and returns the\n",
    "prediction made by the tree (search the documentation of sklearn for details.\n",
    "\n",
    "You can use functions `test_tree` and `test_tree2` to test your\n",
    "implementation and generate results for the report.\n",
    "\n",
    "Implement the function `classify_breast_cancer_with_tree` which using the functions `ii_train_tree` and\n",
    "`ii_test_tree` can distinguish benign and malignant tumor. Use 75% of the dataset for training.\n",
    "\n",
    "\n",
    "Note: for visualizing the trained classifier you need to run `conda install pydotplus python-graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def ii_train_tree(X,Y):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: input matrix of size Nxd, where d is the number of features and N the number of samples\n",
    "    Y: label vector of size N. int values 0 or 1.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model: trained random forest class\n",
    "    \n",
    "    '''\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X,Y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def ii_test_tree(model, X):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: trained random forest class\n",
    "\n",
    "    X: input matrix of size Nxd, where d is the number of features and N the number of samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: output of the logistic regression model\n",
    "    '''    \n",
    "    out =  model.predict(X)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tree():\n",
    "    x_range = [-10, 10]\n",
    "\n",
    "    X, Y = synthetic_lin()\n",
    "    Y = Y * 2 - 1\n",
    "\n",
    "    model = ii_train_tree(X, Y)\n",
    "    x, y, decision_boundaries = decision_boundaries_vec(\n",
    "        x_range, 1000, lambda x: ii_test_tree(model, x)\n",
    "    )\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    axs[0].contourf(x, y, decision_boundaries)\n",
    "    axs[0].scatter(X[Y == -1, 0], X[Y == -1, 1], c='r', marker='*')\n",
    "    axs[0].scatter(X[Y == 1, 0], X[Y == 1, 1], c='b', marker='o')\n",
    "    axs[0].set_title('Tree') \n",
    "\n",
    "    x_range = [-2.5, 2.5]\n",
    "    X, Y = synthetic_nonlin()\n",
    "    Y = Y * 2 - 1\n",
    "\n",
    "    model = ii_train_tree(X,Y)\n",
    "    x, y, decision_boundaries = decision_boundaries_vec(\n",
    "        x_range, 1000, lambda x: ii_test_tree(model, x)\n",
    "    )\n",
    "\n",
    "    axs[1].contourf(x, y, decision_boundaries)\n",
    "    axs[1].scatter(X[Y == -1, 0], X[Y == -1, 1], c='r', marker='*')\n",
    "    axs[1].scatter(X[Y == 1, 0], X[Y == 1, 1], c='b', marker='o')\n",
    "    axs[1].set_title('Tree')\n",
    "\n",
    "    from sklearn.externals.six import StringIO  \n",
    "    from IPython.display import Image  \n",
    "    from sklearn.tree import export_graphviz\n",
    "    import pydotplus\n",
    "    \n",
    "    dot_data = StringIO()\n",
    "    if hasattr(model, 'tree_'):\n",
    "        export_graphviz(model, out_file=dot_data,  \n",
    "                        filled=True, rounded=True,\n",
    "                        special_characters=True)\n",
    "        graph = [pydotplus.graph_from_dot_data(dot_data.getvalue())]\n",
    "    else:\n",
    "        i_tree = 0\n",
    "        for tree_in_forest in model.estimators_:\n",
    "            if (i_tree <3):        \n",
    "                export_graphviz(tree_in_forest, out_file=dot_data)\n",
    "                graph = pydotplus.graph_from_dot_data(dot_data.getvalue())        \n",
    "            i_tree = i_tree + 1\n",
    "    return [g.create_png() for g in graph]\n",
    "\n",
    "\n",
    "def test_tree2():\n",
    "    img, gt = load_graz(0.5)\n",
    "\n",
    "    sy, sx, nc = img.shape\n",
    "\n",
    "    X = create_features(img)\n",
    "\n",
    "    train_columns = sx // 4\n",
    "\n",
    "    true_class = 1\n",
    "\n",
    "    Y = gt\n",
    "\n",
    "    used_train = train_columns * sy\n",
    "\n",
    "    Y = Y.reshape(sy * sx)\n",
    "\n",
    "    tree = ii_train_tree(X[:used_train, :], Y[:used_train])\n",
    "\n",
    "    results = ii_test_tree(tree, X)\n",
    "\n",
    "    results = results.reshape(sy, sx)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "    axs[0].imshow(results, cmap=plt.cm.gray)\n",
    "    axs[0].set_title('Results')\n",
    "    axs[1].imshow(Y.reshape(sy, sx), cmap=plt.cm.gray)\n",
    "    axs[1].set_title('Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "png_list = test_tree2() ### Ask about the Gaphviz package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(png_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_breast_cancer_with_tree(X_train, y_train, X_test):\n",
    "    model = ii_train_tree(X_train,y_train)\n",
    "    predict = ii_test_tree(model,X_test)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Decision Tree Model:  0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data.csv\", header = None)\n",
    "df = df[np.all(df!=\"?\", axis=1)]\n",
    "df = np.array(df)\n",
    "df = df[:,1:]\n",
    "X = df[:,:-1]\n",
    "Y = df[:,df.shape[1]-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.75)\n",
    "model = ii_train_tree(X_train, y_train)\n",
    "y_hat = ii_test_tree(model, X_test)\n",
    "score = f1_score(y_test, y_hat,pos_label=2, average='binary', sample_weight=None, zero_division='warn')\n",
    "print(\"F1 score of the Decision Tree Model: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 --- Cross Validation (2 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is a method to evaluate models. A few common method used for cross validations are:\n",
    "\n",
    "* **Holdout method**: The holdout method is the simplest kind of cross validation. The data set is separated into two sets, called the training set and the testing set. The model is trained on the training set data only. Then the trained model is used to predict the output values for the data in the testing set (it has never seen these output values before). The errors it makes are accumulated to give test set error, which is used to evaluate the model. We used this method in Exercise 4. \n",
    "\n",
    "\n",
    "\n",
    "* **K-fold Cross Validation**: is one way to improve over the holdout method. The data set is divided into k subsets, and the holdout method is repeated k times. Each time, one of the k subsets is used as the test set and the other k-1 subsets are put together to form a training set. Then the average error across all k trials is computed. The advantage of this method is that it matters less how the data gets divided. Every data point gets to be in a test set exactly once, and gets to be in a training set k-1 times. The disadvantage of this method is that the training algorithm has to be rerun from scratch k times, which means it takes k times as much computation to make an evaluation.\n",
    "\n",
    "\n",
    "* **Leave one out cross validation (LOOCV)**: is K-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set. That means that N separate times, the function approximator is trained on all the data except for one point and a prediction is made for that point. As before the average error is computed and used to evaluate the model. The disadvantage of this method is that it is computationally very expensive to compute. \n",
    "\n",
    "\n",
    "* **Stratified K-fold Cross Validation**: Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. For example, in a binary classification problem where each class comprises of 50% of the data, it is best to arrange the data such that in every fold, each class comprises of about half the instances.\n",
    "\n",
    "\n",
    "We will use the dataset from `Exercise 4`. Make sure to load the csv file without header. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data.csv file and load it in a dataframe\n",
    "df = pd.read_csv(\"data/data.csv\", header = None)\n",
    "\n",
    "#show top 5 rows\n",
    "df.head()\n",
    "#check if any missing values\n",
    "df = df[np.all(df!=\"?\", axis=1)] # Removing all missing values\n",
    "np.array(df).shape\n",
    "df = np.array(df)\n",
    "df = df[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Implement function `k_fold` to divide the dataset into k folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(dataset, k, shuffle = False):\n",
    "    # dataset containing the samples\n",
    "    # k is number of folds\n",
    "    # return the generated folds\n",
    "    if shuffle:\n",
    "        np.random.shuffle(dataset)\n",
    "    \n",
    "    N_samples = dataset.shape[0] # Amount of samples\n",
    "    mod = dataset.shape[0]%k # Modulo of Samples to the desired k\n",
    "    if mod == 0: \n",
    "        folds = np.vsplit(dataset,k) ## Does only works for if the dataset is able to divide by k\n",
    "    else:\n",
    "        folds = np.vsplit(dataset[:(N_samples-mod),:],k) ## Split the largest dataset dividable by k\n",
    "        for i in range(mod):\n",
    "            folds[i] = np.vstack((folds[i],dataset[N_samples-mod+i,:])) # Add the rest to the different folds\n",
    "        \n",
    "    \n",
    "        \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Now run `k_fold()` for K=3 and K=5. Then define training and test sets by combining different folds and run the `classify_breast_cancer_with_tree` and report the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classify_k_fold(data,k=2):\n",
    "    folds = k_fold(data,k)\n",
    "    error = []\n",
    "    print(\"Performance for K_fold CV = \",k)\n",
    "    for j in range(0,len(folds)):\n",
    "        df_test = folds[j]\n",
    "        X_test, y_test = df_test[:,:-1],df_test[:,df.shape[1]-1]\n",
    "        temp_fold = folds.copy() # has to be a copy, otherwise pop() removes elements of reference\n",
    "        df_train = np.stack( temp_fold.pop(j), axis=0 )\n",
    "        X_train, y_train = df_train[:,:-1],df_train[:,df.shape[1]-1]\n",
    "        y_hat = classify_breast_cancer_with_tree(X_train, y_train, X_test)\n",
    "        error.append(f1_score(y_test, y_hat,pos_label=2, average='binary', sample_weight=None, zero_division='warn'))\n",
    "        print(f\"F1 Score in test fold {j} =\" , error[j])\n",
    "    \n",
    "    print(\"Average F1 score is\",np.mean(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for K_fold CV =  3\n",
      "F1 Score in test fold 0 = 1.0\n",
      "F1 Score in test fold 1 = 1.0\n",
      "F1 Score in test fold 2 = 1.0\n",
      "Average F1 score is 1.0\n",
      "Performance for K_fold CV =  5\n",
      "F1 Score in test fold 0 = 1.0\n",
      "F1 Score in test fold 1 = 1.0\n",
      "F1 Score in test fold 2 = 1.0\n",
      "F1 Score in test fold 3 = 1.0\n",
      "F1 Score in test fold 4 = 1.0\n",
      "Average F1 score is 1.0\n"
     ]
    }
   ],
   "source": [
    "## Answer here\n",
    "test_classify_k_fold(df,k=3)\n",
    "test_classify_k_fold(df,k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) Complete the function `LOOCV()` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOOCV(dataset):\n",
    "    # dataset containing the samples\n",
    "    \n",
    "    #As to do this on the whole dataset would be time consuming and expensive \n",
    "    #we will use only a small part of the dataset\n",
    "    \n",
    "    #take first 20 rows of the dataset\n",
    "    d = dataset[:20,:]\n",
    "    \n",
    "    #remove the label column\n",
    "    X = d[:,:-1]\n",
    "    Y = d[:,d.shape[1]-1]\n",
    "    \n",
    "    #list for saving predictions\n",
    "    y_hat = np.array([])\n",
    "    #loop over all the dataset with all but one is training \n",
    "    \n",
    "    # I assume you mean one point is for testing\n",
    "    for i in range(len(d)):\n",
    "        \n",
    "        #use classify_breast_cancer_with_tree() to get prediction\n",
    "        X_test, y_test = X[i,:],Y[i]\n",
    "        X_train = np.vstack((X[:i, :], X[i+1:,]))\n",
    "        y_train = np.hstack((Y[:i], Y[i+1:]))\n",
    "        pred = classify_breast_cancer_with_tree(X_train, y_train, X_test.reshape(1, -1))\n",
    "        y_hat = np.append(y_hat,pred)\n",
    "        #add the prediction to the list\n",
    "        \n",
    "    #performance evaluation\n",
    "    difference = np.sum(1*(y_hat != y_test))\n",
    "    print(\"Amount of prediction errors in LOOCV: \",difference,\"\\n\",\"Accuracy: \", 1-difference/len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of prediction errors in LOOCV:  5 \n",
      " Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "LOOCV(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iv) Implement function `k_fold_strat` to divide the dataset into k stratified folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_strat(dataset, k):\n",
    "    # dataset containing the samples\n",
    "    # k is number of folds\n",
    "    # return the generated folds\n",
    "    \n",
    "    \n",
    "    # split dataframe into two dataframes.\n",
    "    # One containing rows with class 2 and\n",
    "    # another with class 4\n",
    "    Y = dataset[:,dataset.shape[1]-1]\n",
    "    data_class2 = dataset[Y==2,:]\n",
    "    data_class4 = dataset[Y==4,:]\n",
    "    # count number of rows in both dataframes\n",
    "\n",
    "    # calculate samples per fold for both class\n",
    "    k_fold_2 = k_fold(data_class2,k)\n",
    "    k_fold_4 = k_fold(data_class4,k)\n",
    "    folds = []\n",
    "    #loop over k to divide the dataframes\n",
    "    for i in range(k):    \n",
    "        folds.append(np.vstack((k_fold_2[i],k_fold_4[i])))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(v) Now run `k_fold_strat()` for K=3 and K=5. Then define training and test sets by combining different folds and run the `classify_breast_cancer_with_tree` and report the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classify_strat_fold(data,k=2):\n",
    "    strat_fold = k_fold_strat(df,k)\n",
    "    error = []\n",
    "    print(\"Performance for K_fold_strat = \",k)\n",
    "    for j in range(0,len(strat_fold)):\n",
    "        df_test = strat_fold[j]\n",
    "        X_test, y_test = df_test[:,:-1],df_test[:,df.shape[1]-1]\n",
    "        temp_fold = strat_fold.copy() # has to be a copy, otherwise pop() removes elements of reference\n",
    "        df_train = np.stack( temp_fold.pop(j), axis=0 )\n",
    "        X_train, y_train = df_train[:,:-1],df_train[:,df.shape[1]-1]\n",
    "        y_hat = classify_breast_cancer_with_tree(X_train, y_train, X_test)\n",
    "        error.append(f1_score(y_test, y_hat,pos_label=2, average='binary', sample_weight=None, zero_division='warn'))\n",
    "        print(f\"F1 Score in test fold {j} =\" , error[j])\n",
    "    \n",
    "    print(\"Average F1 score is\",np.mean(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for K_fold_strat =  3\n",
      "F1 Score in test fold 0 = 1.0\n",
      "F1 Score in test fold 1 = 1.0\n",
      "F1 Score in test fold 2 = 1.0\n",
      "Average F1 score is 1.0\n",
      "Performance for K_fold_strat =  5\n",
      "F1 Score in test fold 0 = 1.0\n",
      "F1 Score in test fold 1 = 1.0\n",
      "F1 Score in test fold 2 = 1.0\n",
      "F1 Score in test fold 3 = 1.0\n",
      "F1 Score in test fold 4 = 1.0\n",
      "Average F1 score is 1.0\n"
     ]
    }
   ],
   "source": [
    "test_classify_strat_fold(df,k=3)\n",
    "test_classify_strat_fold(df,k=5)\n",
    "##Answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
